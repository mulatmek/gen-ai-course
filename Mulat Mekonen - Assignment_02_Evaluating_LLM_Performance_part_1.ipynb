{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cada8d85",
   "metadata": {},
   "source": [
    "# Assignment 2 – Evaluating LLM Output Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a334285",
   "metadata": {},
   "source": [
    "# Introduction to Large Language Models and Prompt Engineering\n",
    "\n",
    "**Course:** GenAI development and LLM applications\n",
    "\n",
    "\n",
    "**Instructors:** Ori Shapira, Yuval Belfer\n",
    "\n",
    "**Semester:** Summer\n",
    "    \n",
    "## Overview\n",
    "\n",
    "This assignment provides a **hands‑on** experince with the world of LLM based systems evaluation: from understanding the business use-case and defining evaluation criterias in light of it. To performing human evaluation and dealing with the hardships of \"non-objectivity\", trough experimenting with **JLMs** (Judge Language Models). \n",
    "\n",
    "Along the way you will explore the differnces between the two evaluation methods, thier advanteges and dis-advanteges and try to figure out how and when to use each further down your GenAI road.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- **Define evaluation criteria** understand the importance of defining how to measure your system performance in a non closely defined problem.\n",
    "- **Compare** manual vs. automatic common methods.\n",
    "- **Drive improvement** through proper evaluation, documentation and change cycles.\n",
    "- **Design** usable automatic evaluation pipeline.\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Familiarity with Jupyter notebooks\n",
    "- Internet connection for API calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f3462",
   "metadata": {},
   "source": [
    "# Part 1 - Human evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a26e883",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee74b55b",
   "metadata": {},
   "source": [
    "## 1  Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e10427",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade \"transformers[torch]\" datasets accelerate bitsandbytes --progress-bar off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef754d5c",
   "metadata": {},
   "source": [
    "## 2  Business use case – Generate Product Descriptions\n",
    "Many e‑commerce sites need engaging **product descriptions**. Given structured attributes (name, category, features, color, price), your model should craft a persuasive, 50‑90‑word description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6debe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 products\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>Product_attribute_list</th>\n",
       "      <th>material</th>\n",
       "      <th>warranty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple iPhone 15 Pro</td>\n",
       "      <td>features: A17 Pro chip, 120 Hz ProMotion displ...</td>\n",
       "      <td>titanium frame, Ceramic Shield glass</td>\n",
       "      <td>1‑year limited warranty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samsung Galaxy S24 Ultra</td>\n",
       "      <td>features: 200 MP camera, S‑Pen support, 120 Hz...</td>\n",
       "      <td>Armor Aluminum frame, Gorilla Glass Victus</td>\n",
       "      <td>1‑year limited warranty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Pixel 8 Pro</td>\n",
       "      <td>features: Tensor G3 chip, Magic Eraser, 50 MP ...</td>\n",
       "      <td>matte glass back, aluminum frame</td>\n",
       "      <td>1‑year limited warranty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sony WH‑1000XM5 Headphones</td>\n",
       "      <td>features: active noise cancelling, 30 hr batte...</td>\n",
       "      <td>synthetic leather earcups</td>\n",
       "      <td>1‑year limited warranty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bose QuietComfort Ultra Earbuds</td>\n",
       "      <td>features: CustomTune sound calibration, ANC, I...</td>\n",
       "      <td>silicone ear tips</td>\n",
       "      <td>1‑year limited warranty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      product_name  \\\n",
       "0              Apple iPhone 15 Pro   \n",
       "1         Samsung Galaxy S24 Ultra   \n",
       "2               Google Pixel 8 Pro   \n",
       "3       Sony WH‑1000XM5 Headphones   \n",
       "4  Bose QuietComfort Ultra Earbuds   \n",
       "\n",
       "                              Product_attribute_list  \\\n",
       "0  features: A17 Pro chip, 120 Hz ProMotion displ...   \n",
       "1  features: 200 MP camera, S‑Pen support, 120 Hz...   \n",
       "2  features: Tensor G3 chip, Magic Eraser, 50 MP ...   \n",
       "3  features: active noise cancelling, 30 hr batte...   \n",
       "4  features: CustomTune sound calibration, ANC, I...   \n",
       "\n",
       "                                     material                 warranty  \n",
       "0        titanium frame, Ceramic Shield glass  1‑year limited warranty  \n",
       "1  Armor Aluminum frame, Gorilla Glass Victus  1‑year limited warranty  \n",
       "2            matte glass back, aluminum frame  1‑year limited warranty  \n",
       "3                   synthetic leather earcups  1‑year limited warranty  \n",
       "4                           silicone ear tips  1‑year limited warranty  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the product dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"Assignment_02_product_dataset.csv\"  # ensure the file is uploaded\n",
    "df_products = pd.read_csv(dataset_path)\n",
    "print(f\"Loaded {len(df_products)} products\")\n",
    "df_products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d4731",
   "metadata": {},
   "source": [
    "## 3  Evaluation criteria\n",
    "| Criterion | Description | Rating |\n",
    "|-----------|-------------|--------|\n",
    "| **Fluency** | Natural, easy‑to‑read sentences | good / ok / bad |\n",
    "| **Grammar** | Correct spelling & punctuation | good / ok / bad |\n",
    "| **Tone** | Matches friendly, credible sales voice | good / ok / bad |\n",
    "| **Length** | 50‑90 words | good / ok / bad |\n",
    "| **Grounding** | Sticks to provided attributes only | good / ok / bad |\n",
    "| **Latency** | Time to first byte / full response | good / ok / bad  (based on avg. time per call)|\n",
    "| **Cost** | Relative inference or API cost per 1K tokens | good / ok / bad (based on avg. price per cal)|\n",
    "\n",
    "**Define your rubric:**\n",
    "1. For each criterion, spell out what qualifies as **good**, **ok**, and **bad** to minimize subjectivity (e.g. for *Length*: good = 50‑90 words, ok = 40‑49 or 91‑110 words, bad = outside that range).\n",
    "2. Decide the **cumulative pass bar**—for instance, at least three *good* ratings and no *bad* ratings overall.\n",
    "3. Establish **go / no‑go rules**—e.g. if *Grounding* is *bad* the description is automatically rejected, regardless of other scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ea603",
   "metadata": {},
   "source": [
    "## 4  Prompt\n",
    "\n",
    "💡 **Prompt‑engineering tip:**\n",
    "Feel free to iterate on the prompt to maximize output quality. You can:\n",
    "- Add a **system message** that defines writing style, brand voice, or formatting rules.\n",
    "- Provide **one or two high‑quality examples** (few‑shot) of attribute→description pairs.\n",
    "- Include explicit constraints (word count, tone adjectives, banned phrases).\n",
    "- Experiment with phrases like *\"Think step‑by‑step\"* or *\"First reason, then answer\"*.\n",
    "\n",
    "Document any changes you make and observe how they influence the evaluation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebfeb600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\M'\n",
      "/var/folders/8k/xt0vhllx4jj6srjchtkrplr40000gn/T/ipykernel_67322/629265950.py:4: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  \"Product name: {product_name}\\nFeatures: {Product_attribute_list}\\Material: {material}\\nWarranty: {warranty}\\n\\n\"\n"
     ]
    }
   ],
   "source": [
    "prompt_tmpl = (\n",
    "    \"You are a copywriter for an online store. Using the product attributes, \"\n",
    "    \"write an engaging product description (50–90 words).\\n\\n\"\n",
    "    \"Product name: {product_name}\\nFeatures: {Product_attribute_list}\\Material: {material}\\nWarranty: {warranty}\\n\\n\"\n",
    "    \"Description:\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a920775e",
   "metadata": {},
   "source": [
    "## 5  Run a medium‑size model (≤ 30 B parameters)\n",
    "\n",
    "Choose **one or more** of the options below:\n",
    "\n",
    "**A. Hugging Face checkpoint** (local inference) – already configured in the code cell that follows.\n",
    "\n",
    "**B. OpenAI model** – call an OpenAI hosted model (e.g. `gpt‑4o`, `gpt‑4‑turbo`, `gpt‑3.5‑turbo`). Implement `call_openai(prompt: str) -> str` in a separate utility cell and then run the snippet.\n",
    "\n",
    "**C. Google Gemini model** – call a Gemini endpoint (e.g. `gemini‑1.5‑pro`). Implement `call_gemini(prompt: str) -> str` similarly.\n",
    "\n",
    "> ⚠️ Make sure you have your API keys set as environment variables or passed securely.\n",
    "\n",
    "\n",
    "**Latency & cost tracking**\n",
    "- Your `call_*` functions should return a **dict** with keys:\n",
    "  - `text` – generated description (string)\n",
    "  - `latency_ms` – end‑to‑end generation time in milliseconds\n",
    "  - `input_tokens` – tokens sent to the model (**IF YOU ADDED A SYS PROMPT ADD IT TO THE CALCULATION**)\n",
    "  - `output_tokens` – tokens received from the model\n",
    "- Below, a helper `call_hf()` shows how to compute these metrics for a Hugging Face model. You must add equivalent tracking inside `call_openai()` and `call_gemini()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0689cc37",
   "metadata": {},
   "source": [
    "**FOR COLAB USERS**\n",
    "\n",
    "You can set your HF_TOKEN secret in Colab, please follow these steps:\n",
    "\n",
    "1. Click on the \"🔑\" icon in the left-hand sidebar in Colab. This opens the Secrets manager.\n",
    "2. Click on \"New secret\".\n",
    "3. In the \"Name\" field, type HF_TOKEN.\n",
    "4. In the \"Value\" field, paste your Hugging Face access token (you can generate one from your Hugging Face account settings under \"Access Tokens\").\n",
    "5. Make sure the \"Notebook access\" toggle is enabled for your notebook.\n",
    "6. Close the Secrets manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1902bf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR_HF_TOKEN\n"
     ]
    }
   ],
   "source": [
    "# Set your Hugging Face access token\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"YOUR_HF_TOKEN\"\n",
    "print(os.environ.get(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b73a9cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimchen/anaconda3/envs/new-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Switch Colab to GPU first",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSwitch Colab to GPU first\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m bnb \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Switch Colab to GPU first"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "\n",
    "assert torch.cuda.is_available(), \"Switch Colab to GPU first\"\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "bnb = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "# this will work with L4 GPU, if you have a different GPU, you may need to modify the code\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    max_memory={0: \"24GiB\", \"cpu\": \"30GiB\"},\n",
    "    quantization_config=bnb,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "hf_tok = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "hf_gen = pipeline(\"text-generation\",\n",
    "               model=hf_model,\n",
    "               tokenizer=hf_tok,\n",
    "               max_new_tokens=120,\n",
    "               do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5415b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dict(\n",
    "    product_name=\"EverCool Water Bottle\",\n",
    "    Product_attribute_list=\"double-wall vacuum insulation; keeps drinks cold 24 h; leak-proof lid\",\n",
    "    material=\"stainless steel\",\n",
    "    warranty=\"lifetime warranty\",\n",
    ")\n",
    "\n",
    "print(hf_gen(prompt_tmpl.format(**sample),\n",
    "          return_full_text=False)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f09463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Option A: Hugging Face ---\n",
    "# Ensure you implemented call_hf() in another cell (you can use the one from the previous assignment).\n",
    "\n",
    "# Example with HF pipeline\n",
    "def call_hf(prompt: str, model_id: str = \"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    \"\"\"Return dict with latency & token counts for HF pipeline.\"\"\"\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_8bit=True)\n",
    "    # generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=120)\n",
    "\n",
    "    import time\n",
    "    start = time.time()\n",
    "    res= hf_gen(prompt,\n",
    "          return_full_text=False)[0][\"generated_text\"]\n",
    "\n",
    "    latency = (time.time() - start) * 1000  # ms\n",
    "    # token counts via tokenizer\n",
    "    input_tokens = len(hf_tok.encode(prompt))\n",
    "    output_tokens = len(hf_tok.encode(res))\n",
    "    return {\n",
    "        \"text\": res,\n",
    "        \"latency_ms\": latency,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "    }\n",
    "\n",
    "response_hf = call_hf(prompt_tmpl.format(**sample))\n",
    "print(response_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69bcdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Option B: OpenAI ---\n",
    "import openai\n",
    "import time\n",
    "\n",
    "def call_openai(prompt: str, model_name: str = \"gpt-4o\", api_key: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Call OpenAI API and return generated text and usage metrics.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user prompt.\n",
    "        model_name (str): OpenAI model to use (default: \"gpt-4o\").\n",
    "        api_key (str): OpenAI API Key (optional if already set globally).\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"text\": generated description (str),\n",
    "            \"latency_ms\": latency in milliseconds (float),\n",
    "            \"input_tokens\": tokens sent (int),\n",
    "            \"output_tokens\": tokens received (int)\n",
    "        }\n",
    "    \"\"\"\n",
    "    if api_key:\n",
    "        openai.api_key = api_key\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        latency_ms = (end_time - start_time) * 1000\n",
    "\n",
    "        output = response.choices[0].message.content.strip()\n",
    "        usage = response.usage\n",
    "\n",
    "        return {\n",
    "            \"text\": output,\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"input_tokens\": usage.prompt_tokens if usage else None,\n",
    "            \"output_tokens\": usage.completion_tokens if usage else None\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API Error: {e}\")\n",
    "        return {\n",
    "            \"text\": \"\",\n",
    "            \"latency_ms\": 0,\n",
    "            \"input_tokens\": 0,\n",
    "            \"output_tokens\": 0\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c3ef143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access variables\n",
    "gemini_api_key = os.getenv(\"gemini_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69cc3142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Option C: Gemini ---\n",
    "def call_gemini(prompt: str, model_name: str = \"models/gemini-1.5-pro\", api_key: str = gemini_api_key) -> dict:\n",
    "    \"\"\"\n",
    "    Call Google Gemini API and return generated text and usage metrics.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user prompt.\n",
    "        model_name (str): Gemini model to use (default: \"gemini-pro\").\n",
    "        api_key (str): Google API Key.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"text\": generated description (str),\n",
    "            \"latency_ms\": latency in milliseconds (float),\n",
    "            \"input_tokens\": tokens sent (int),\n",
    "            \"output_tokens\": tokens received (int)\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    if not api_key:\n",
    "        raise ValueError(\"api_key must be provided for Google Gemini API\")\n",
    "\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "    # Initialize model\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "\n",
    "    # Measure start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        end_time = time.time()\n",
    "        latency_ms = (end_time - start_time) * 1000\n",
    "\n",
    "        text = response.text.strip() if hasattr(response, \"text\") else \"\"\n",
    "\n",
    "        # Token accounting (Gemini SDK does not expose usage directly yet)\n",
    "        input_tokens = len(prompt.split())\n",
    "        output_tokens = len(text.split())\n",
    "\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini API Error: {e}\")\n",
    "        return {\n",
    "            \"text\": \"\",\n",
    "            \"latency_ms\": 0,\n",
    "            \"input_tokens\": 0,\n",
    "            \"output_tokens\": 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a89bdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001 ['embedText', 'countTextTokens']\n",
      "models/gemini-1.0-pro-vision-latest ['generateContent', 'countTokens']\n",
      "models/gemini-pro-vision ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-pro-latest ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-pro-002 ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-pro ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-latest ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-002 ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-flash-8b ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-001 ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-latest ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-2.5-pro-preview-03-25 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-05-20 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite-preview-06-17 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro-preview-05-06 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro-preview-06-05 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-exp ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-001 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-exp-image-generation ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash-lite-001 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-preview-image-generation ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-lite-preview-02-05 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite-preview ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-pro-exp ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-pro-exp-02-05 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-exp-1206 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp-01-21 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp-1219 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-tts ['countTokens', 'generateContent']\n",
      "models/gemini-2.5-pro-preview-tts ['countTokens', 'generateContent']\n",
      "models/learnlm-2.0-flash-experimental ['generateContent', 'countTokens']\n",
      "models/gemma-3-1b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-4b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-12b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-27b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e4b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e2b-it ['generateContent', 'countTokens']\n",
      "models/gemini-2.5-flash-lite ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/embedding-001 ['embedContent']\n",
      "models/text-embedding-004 ['embedContent']\n",
      "models/gemini-embedding-exp-03-07 ['embedContent', 'countTextTokens', 'countTokens']\n",
      "models/gemini-embedding-exp ['embedContent', 'countTextTokens', 'countTokens']\n",
      "models/gemini-embedding-001 ['embedContent', 'countTextTokens', 'countTokens']\n",
      "models/aqa ['generateAnswer']\n",
      "models/imagen-3.0-generate-002 ['predict']\n",
      "models/imagen-4.0-generate-preview-06-06 ['predict']\n",
      "models/imagen-4.0-ultra-generate-preview-06-06 ['predict']\n",
      "models/veo-2.0-generate-001 ['predictLongRunning']\n",
      "models/veo-3.0-generate-preview ['predictLongRunning']\n",
      "models/gemini-2.5-flash-preview-native-audio-dialog ['countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.5-flash-exp-native-audio-thinking-dialog ['countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash-live-001 ['bidiGenerateContent', 'countTokens']\n",
      "models/gemini-live-2.5-flash-preview ['bidiGenerateContent', 'countTokens']\n",
      "models/gemini-2.5-flash-live-preview ['bidiGenerateContent', 'countTokens']\n"
     ]
    }
   ],
   "source": [
    "models = genai.list_models()\n",
    "for m in models:\n",
    "    print(m.name, m.supported_generation_methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "299fc02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>Product_attribute_list</th>\n",
       "      <th>material</th>\n",
       "      <th>warranty</th>\n",
       "      <th>generated_description</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple iPhone 15 Pro</td>\n",
       "      <td>features: A17 Pro chip, 120 Hz ProMotion displ...</td>\n",
       "      <td>titanium frame, Ceramic Shield glass</td>\n",
       "      <td>1‑year limited warranty</td>\n",
       "      <td>Experience the power of the Apple iPhone 15 Pr...</td>\n",
       "      <td>2331.658125</td>\n",
       "      <td>49</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samsung Galaxy S24 Ultra</td>\n",
       "      <td>features: 200 MP camera, S‑Pen support, 120 Hz...</td>\n",
       "      <td>Armor Aluminum frame, Gorilla Glass Victus</td>\n",
       "      <td>1‑year limited warranty</td>\n",
       "      <td>Capture brilliance with the Samsung Galaxy S24...</td>\n",
       "      <td>2267.369032</td>\n",
       "      <td>48</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Pixel 8 Pro</td>\n",
       "      <td>features: Tensor G3 chip, Magic Eraser, 50 MP ...</td>\n",
       "      <td>matte glass back, aluminum frame</td>\n",
       "      <td>1‑year limited warranty</td>\n",
       "      <td>Experience the power of the Google Pixel 8 Pro...</td>\n",
       "      <td>2253.291845</td>\n",
       "      <td>47</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sony WH‑1000XM5 Headphones</td>\n",
       "      <td>features: active noise cancelling, 30 hr batte...</td>\n",
       "      <td>synthetic leather earcups</td>\n",
       "      <td>1‑year limited warranty</td>\n",
       "      <td>Immerse yourself in pure sound with the Sony W...</td>\n",
       "      <td>1943.329096</td>\n",
       "      <td>44</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bose QuietComfort Ultra Earbuds</td>\n",
       "      <td>features: CustomTune sound calibration, ANC, I...</td>\n",
       "      <td>silicone ear tips</td>\n",
       "      <td>1‑year limited warranty</td>\n",
       "      <td>Experience world-class sound and silence with ...</td>\n",
       "      <td>1944.385052</td>\n",
       "      <td>42</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      product_name  \\\n",
       "0              Apple iPhone 15 Pro   \n",
       "1         Samsung Galaxy S24 Ultra   \n",
       "2               Google Pixel 8 Pro   \n",
       "3       Sony WH‑1000XM5 Headphones   \n",
       "4  Bose QuietComfort Ultra Earbuds   \n",
       "\n",
       "                              Product_attribute_list  \\\n",
       "0  features: A17 Pro chip, 120 Hz ProMotion displ...   \n",
       "1  features: 200 MP camera, S‑Pen support, 120 Hz...   \n",
       "2  features: Tensor G3 chip, Magic Eraser, 50 MP ...   \n",
       "3  features: active noise cancelling, 30 hr batte...   \n",
       "4  features: CustomTune sound calibration, ANC, I...   \n",
       "\n",
       "                                     material                 warranty  \\\n",
       "0        titanium frame, Ceramic Shield glass  1‑year limited warranty   \n",
       "1  Armor Aluminum frame, Gorilla Glass Victus  1‑year limited warranty   \n",
       "2            matte glass back, aluminum frame  1‑year limited warranty   \n",
       "3                   synthetic leather earcups  1‑year limited warranty   \n",
       "4                           silicone ear tips  1‑year limited warranty   \n",
       "\n",
       "                               generated_description   latency_ms  \\\n",
       "0  Experience the power of the Apple iPhone 15 Pr...  2331.658125   \n",
       "1  Capture brilliance with the Samsung Galaxy S24...  2267.369032   \n",
       "2  Experience the power of the Google Pixel 8 Pro...  2253.291845   \n",
       "3  Immerse yourself in pure sound with the Sony W...  1943.329096   \n",
       "4  Experience world-class sound and silence with ...  1944.385052   \n",
       "\n",
       "   input_tokens  output_tokens  \n",
       "0            49             57  \n",
       "1            48             62  \n",
       "2            47             58  \n",
       "3            44             52  \n",
       "4            42             55  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Batch generation helper (type‑safe) ---\n",
    "from typing import Callable\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Callable, Dict\n",
    "import pandas as pd\n",
    "\n",
    "def batch_generate(\n",
    "    sample_df: pd.DataFrame,\n",
    "    call_model_fn: Callable[[str], Dict[str, object]],\n",
    "    prompt_template: str = prompt_tmpl,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Generate descriptions and metrics for each row in *sample_df*.\n",
    "\n",
    "    The model-calling function *must* return a dict with keys:\n",
    "    - ``text`` (str) – generated description\n",
    "    - ``latency_ms`` (float | None)\n",
    "    - ``input_tokens`` (int | None)\n",
    "    - ``output_tokens`` (int | None)\n",
    "    \"\"\"\n",
    "    if not isinstance(sample_df, pd.DataFrame):\n",
    "        raise TypeError(\"sample_df must be a pandas DataFrame\")\n",
    "    if not callable(call_model_fn):\n",
    "        raise TypeError(\"call_model_fn must be callable\")\n",
    "\n",
    "    outputs = []\n",
    "    for _, row in sample_df.iterrows():\n",
    "        prompt = prompt_template.format(**row.to_dict())\n",
    "        out = call_model_fn(prompt)\n",
    "        if not isinstance(out, dict) or 'text' not in out:\n",
    "            raise ValueError(\"call_model_fn must return a dict with at least a 'text' field\")\n",
    "        outputs.append(out)\n",
    "\n",
    "    result_df = sample_df.copy()\n",
    "    result_df[\"generated_description\"] = [o[\"text\"] for o in outputs]\n",
    "    result_df[\"latency_ms\"] = [o.get(\"latency_ms\") for o in outputs]\n",
    "    result_df[\"input_tokens\"] = [o.get(\"input_tokens\") for o in outputs]\n",
    "    result_df[\"output_tokens\"] = [o.get(\"output_tokens\") for o in outputs]\n",
    "    return result_df\n",
    "\n",
    "\n",
    "demo_df = batch_generate(df_products[:5], call_gemini)\n",
    "demo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87759e4a",
   "metadata": {},
   "source": [
    "## 6  Manual evaluation\n",
    "Use `batch_generate()` to create a DataFrame of model outputs, then add blank rating columns for each criterion plus a `final_score` column. An Excel file is saved so you can fill scores offline or share with peers.\n",
    "\n",
    "Steps:\n",
    "1. Run the code cell below (adjust which `call_*` function you pass in).\n",
    "2. Open the generated `assignment_03_evaluation_sheet.xlsx` and rate each row with **good / ok / bad**.\n",
    "3. Add a rule for `final_score` (e.g., majority = good, fails if grounding = bad).\n",
    "\n",
    "\n",
    "**Cost calculator**\n",
    "Use the helper below to compute cost in USD based on token usage:\n",
    "```python\n",
    "outputs_df = add_cost_columns(outputs_df, input_price_per_m=1.5, output_price_per_m=2.0)\n",
    "```\n",
    "Set prices to **0** if you ran everything locally on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b271623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cost computation helper ---\n",
    "def add_cost_columns(df, input_price_per_m: float, output_price_per_m: float):\n",
    "    \"\"\"Add cost columns based on token counts.\n",
    "    Args:\n",
    "        df: DataFrame with `input_tokens` and `output_tokens`.\n",
    "        input_price_per_m: $ per 1M input tokens.\n",
    "        output_price_per_m: $ per 1M output tokens.\n",
    "    Returns: DataFrame with extra `cost_usd` column.\n",
    "    \"\"\"\n",
    "    if 'input_tokens' not in df or 'output_tokens' not in df:\n",
    "        raise ValueError('Token columns missing; run batch_generate first')\n",
    "    cost_input = df['input_tokens'] * (input_price_per_m / 1000000)\n",
    "    cost_output = df['output_tokens'] * (output_price_per_m / 1000000)\n",
    "    df = df.copy()\n",
    "    df['cost_usd'] = (cost_input + cost_output).round(4)\n",
    "    return df\n",
    "\n",
    "# Example usage (set prices to 0 for HF local models):\n",
    "# outputs_df = add_cost_columns(outputs_df, 0, 0)\n",
    "# outputs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c66c1fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation sheet → assignment_03_evaluation_sheet.xlsx with 50 rows\n"
     ]
    }
   ],
   "source": [
    "# --- Build evaluation sheet & export to Excel ---\n",
    "\n",
    "#Update the prices according to the model you used, or leave them at 0 for HF local models\n",
    "YOUR_MODEL_INPUT_PRICE_PER_M = 0\n",
    "YOUR_MODEL_OUTPUT_PRICE_PER_M = 0\n",
    "outputs_df = batch_generate(df_products, call_gemini)  # NOTE: change model function as needed\n",
    "\n",
    "# Add rating columns (good/ok/bad)\n",
    "rating_cols = [\"fluency\", \"grammar\", \"tone\", \"length\", \"grounding\", \"latency\", \"cost\", \"final_score\"]\n",
    "for col in rating_cols:\n",
    "    if col not in outputs_df:\n",
    "        outputs_df[col] = \"\"\n",
    "\n",
    "xlsx_path = \"assignment_03_evaluation_sheet.xlsx\"\n",
    "\n",
    "# Add cost columns\n",
    "outputs_df = add_cost_columns(outputs_df, YOUR_MODEL_INPUT_PRICE_PER_M, YOUR_MODEL_OUTPUT_PRICE_PER_M)\n",
    "\n",
    "outputs_df.to_excel(xlsx_path, index=False)\n",
    "print(f\"Saved evaluation sheet → {xlsx_path} with {len(outputs_df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b78ecd",
   "metadata": {},
   "source": [
    "## 7  Improvement cycle\n",
    "\n",
    "Now that you’ve established a baseline score in **Section 6**, iterate to achieve better results.\n",
    "\n",
    "**Ideas to explore**\n",
    "- **Prompt tuning** – rewrite the system/user prompts, add few‑shot examples, or enforce stricter constraints.\n",
    "- **Model choice** – test a different checkpoint (larger ≠ always better), switch from HF to OpenAI or Gemini, or try a domain‑specific model.\n",
    "- **Temperature / decoding params** – adjust `temperature`, `top_p`, `top_k`, or `max_new_tokens` to balance creativity vs. factuality.\n",
    "- **Data preprocessing** – clean attribute text, expand abbreviations, or group similar products to feed additional context.\n",
    "- **Post‑processing** – run grammar‑checking or length trimming after generation.\n",
    "- **Ensembling / RAG** – combine outputs from two models or ground the prompt with retrieved copy from existing catalog listings.\n",
    "\n",
    "Document each experiment in a brief bullet list:\n",
    "1. **What you changed**\n",
    "2. **Why you expected it to help**\n",
    "3. **New evaluation scores**\n",
    "\n",
    "💡 *Goal*: maximize the cumulative score according to your rubric while respecting the go/no‑go rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7c4c7907",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tmpl = (\n",
    "    \"You are a professional copywriter for a premium online tech store. \"\n",
    "    \"Using the product details below, write a compelling and benefit-driven product description in **exactly 50 to 90 words**. \"\n",
    "    \"Do not exceed this word range — keep it concise, natural, and persuasive. \"\n",
    "    \"Highlight benefits over features, and integrate all product attributes seamlessly. \"\n",
    "    \"Use a confident, modern tone that appeals to tech-savvy customers.\\n\\n\"\n",
    "    \"Product name: {product_name}\\n\"\n",
    "    \"Key features: {Product_attribute_list}\\n\"\n",
    "    \"Material: {material}\\n\"\n",
    "    \"Warranty: {warranty}\\n\\n\"\n",
    "    \"Only return the product description (no title, no bullet points, no word count). Begin directly with the description:\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7920b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Option C: Gemini ---\n",
    "def call_gemini(\n",
    "    prompt: str,\n",
    "    model_name: str = \"models/gemini-1.5-pro\",\n",
    "    api_key: str = gemini_api_key,\n",
    "    temperature: float = 0.85,\n",
    "    top_p: float = 0.95,\n",
    "    top_k: int = 50\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Call Google Gemini API and return generated text and usage metrics.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user prompt.\n",
    "        model_name (str): Gemini model to use.\n",
    "        api_key (str): Google API Key.\n",
    "        temperature (float): Controls randomness.\n",
    "        top_p (float): Controls nucleus sampling.\n",
    "        top_k (int): Limits candidates to top-k tokens by probability.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"text\": generated description (str),\n",
    "            \"latency_ms\": latency in milliseconds (float),\n",
    "            \"input_tokens\": tokens sent (int),\n",
    "            \"output_tokens\": tokens received (int)\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    if not api_key:\n",
    "        raise ValueError(\"api_key must be provided for Google Gemini API\")\n",
    "\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "    # Initialize model\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "\n",
    "    # Measure start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k\n",
    "            )\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        latency_ms = (end_time - start_time) * 1000\n",
    "\n",
    "        text = response.text.strip() if hasattr(response, \"text\") else \"\"\n",
    "\n",
    "        # Token accounting (Gemini SDK does not expose usage directly yet)\n",
    "        input_tokens = len(prompt.split())\n",
    "        output_tokens = len(text.split())\n",
    "\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini API Error: {e}\")\n",
    "        return {\n",
    "            \"text\": \"\",\n",
    "            \"latency_ms\": 0,\n",
    "            \"input_tokens\": 0,\n",
    "            \"output_tokens\": 0\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25a1284c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>Product_attribute_list</th>\n",
       "      <th>material</th>\n",
       "      <th>warranty</th>\n",
       "      <th>generated_description</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple iPhone 15 Pro</td>\n",
       "      <td>features: A17 Pro chip, 120 Hz ProMotion displ...</td>\n",
       "      <td>titanium frame, Ceramic Shield glass</td>\n",
       "      <td>1‑year limited warranty</td>\n",
       "      <td>Experience blistering performance with the iPh...</td>\n",
       "      <td>2446.162939</td>\n",
       "      <td>111</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samsung Galaxy S24 Ultra</td>\n",
       "      <td>features: 200 MP camera, S‑Pen support, 120 Hz...</td>\n",
       "      <td>Armor Aluminum frame, Gorilla Glass Victus</td>\n",
       "      <td>1‑year limited warranty</td>\n",
       "      <td>Capture brilliance with the Samsung Galaxy S24...</td>\n",
       "      <td>2200.408936</td>\n",
       "      <td>110</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Pixel 8 Pro</td>\n",
       "      <td>features: Tensor G3 chip, Magic Eraser, 50 MP ...</td>\n",
       "      <td>matte glass back, aluminum frame</td>\n",
       "      <td>1‑year limited warranty</td>\n",
       "      <td>Experience the next evolution of mobile with t...</td>\n",
       "      <td>2273.229837</td>\n",
       "      <td>109</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sony WH‑1000XM5 Headphones</td>\n",
       "      <td>features: active noise cancelling, 30 hr batte...</td>\n",
       "      <td>synthetic leather earcups</td>\n",
       "      <td>1‑year limited warranty</td>\n",
       "      <td>Experience sound redefined with the Sony WH-10...</td>\n",
       "      <td>2089.838028</td>\n",
       "      <td>106</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bose QuietComfort Ultra Earbuds</td>\n",
       "      <td>features: CustomTune sound calibration, ANC, I...</td>\n",
       "      <td>silicone ear tips</td>\n",
       "      <td>1‑year limited warranty</td>\n",
       "      <td>Experience world-class sound personalized for ...</td>\n",
       "      <td>2394.515991</td>\n",
       "      <td>104</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      product_name  \\\n",
       "0              Apple iPhone 15 Pro   \n",
       "1         Samsung Galaxy S24 Ultra   \n",
       "2               Google Pixel 8 Pro   \n",
       "3       Sony WH‑1000XM5 Headphones   \n",
       "4  Bose QuietComfort Ultra Earbuds   \n",
       "\n",
       "                              Product_attribute_list  \\\n",
       "0  features: A17 Pro chip, 120 Hz ProMotion displ...   \n",
       "1  features: 200 MP camera, S‑Pen support, 120 Hz...   \n",
       "2  features: Tensor G3 chip, Magic Eraser, 50 MP ...   \n",
       "3  features: active noise cancelling, 30 hr batte...   \n",
       "4  features: CustomTune sound calibration, ANC, I...   \n",
       "\n",
       "                                     material                 warranty  \\\n",
       "0        titanium frame, Ceramic Shield glass  1‑year limited warranty   \n",
       "1  Armor Aluminum frame, Gorilla Glass Victus  1‑year limited warranty   \n",
       "2            matte glass back, aluminum frame  1‑year limited warranty   \n",
       "3                   synthetic leather earcups  1‑year limited warranty   \n",
       "4                           silicone ear tips  1‑year limited warranty   \n",
       "\n",
       "                               generated_description   latency_ms  \\\n",
       "0  Experience blistering performance with the iPh...  2446.162939   \n",
       "1  Capture brilliance with the Samsung Galaxy S24...  2200.408936   \n",
       "2  Experience the next evolution of mobile with t...  2273.229837   \n",
       "3  Experience sound redefined with the Sony WH-10...  2089.838028   \n",
       "4  Experience world-class sound personalized for ...  2394.515991   \n",
       "\n",
       "   input_tokens  output_tokens  \n",
       "0           111             62  \n",
       "1           110             66  \n",
       "2           109             65  \n",
       "3           106             57  \n",
       "4           104             51  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Batch generation helper (type‑safe) ---\n",
    "from typing import Callable\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Callable, Dict\n",
    "import pandas as pd\n",
    "\n",
    "def batch_generate(\n",
    "    sample_df: pd.DataFrame,\n",
    "    call_model_fn: Callable[[str], Dict[str, object]],\n",
    "    prompt_template: str = prompt_tmpl,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Generate descriptions and metrics for each row in *sample_df*.\n",
    "\n",
    "    The model-calling function *must* return a dict with keys:\n",
    "    - ``text`` (str) – generated description\n",
    "    - ``latency_ms`` (float | None)\n",
    "    - ``input_tokens`` (int | None)\n",
    "    - ``output_tokens`` (int | None)\n",
    "    \"\"\"\n",
    "    if not isinstance(sample_df, pd.DataFrame):\n",
    "        raise TypeError(\"sample_df must be a pandas DataFrame\")\n",
    "    if not callable(call_model_fn):\n",
    "        raise TypeError(\"call_model_fn must be callable\")\n",
    "\n",
    "    outputs = []\n",
    "    for _, row in sample_df.iterrows():\n",
    "        prompt = prompt_template.format(**row.to_dict())\n",
    "        out = call_model_fn(prompt)\n",
    "        if not isinstance(out, dict) or 'text' not in out:\n",
    "            raise ValueError(\"call_model_fn must return a dict with at least a 'text' field\")\n",
    "        outputs.append(out)\n",
    "\n",
    "    result_df = sample_df.copy()\n",
    "    result_df[\"generated_description\"] = [o[\"text\"] for o in outputs]\n",
    "    result_df[\"latency_ms\"] = [o.get(\"latency_ms\") for o in outputs]\n",
    "    result_df[\"input_tokens\"] = [o.get(\"input_tokens\") for o in outputs]\n",
    "    result_df[\"output_tokens\"] = [o.get(\"output_tokens\") for o in outputs]\n",
    "    return result_df\n",
    "\n",
    "\n",
    "demo_df = batch_generate(df_products[:5], call_gemini)\n",
    "demo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0426306d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation sheet → assignment_03_improvement_sheet.xlsx with 5 rows\n"
     ]
    }
   ],
   "source": [
    "# --- Build improvement sheet & export to Excel ---\n",
    "\n",
    "outputs_df = batch_generate(df_products.head(), call_gemini)  # NOTE: change model function as needed\n",
    "\n",
    "\n",
    "xlsx_path = \"assignment_03_improvement_sheet.xlsx\"\n",
    "\n",
    "outputs_df.to_excel(xlsx_path, index=False)\n",
    "print(f\"Saved evaluation sheet → {xlsx_path} with {len(outputs_df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ab376a",
   "metadata": {},
   "source": [
    "# Part 2 – Judging Language Models (JLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5e933775",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompt_tmpl = \"\"\"\n",
    "You are an expert evaluator of marketing copy. Based on the product information and the generated description below, evaluate the description using **only** these labels: \"good\", \"ok\", or \"bad\".\n",
    "\n",
    "Rate each of the following categories:\n",
    "- fluency\n",
    "- grammar\n",
    "- tone\n",
    "- length (should be between 50–90 words)\n",
    "- grounding (faithfulness to product attributes)\n",
    "- latency (perceived delivery speed / verbosity)\n",
    "- cost (efficiency of wording)\n",
    "- final_score (overall quality)\n",
    "\n",
    "**Product Details**\n",
    "- Product name: {product_name}\n",
    "- Key features: {Product_attribute_list}\n",
    "- Material: {material}\n",
    "- Warranty: {warranty}\n",
    "\n",
    "**Generated Description**\n",
    "{generated_description}\n",
    "\n",
    "Return your evaluation in the following plain text format:\n",
    "\n",
    "fluency: <good|ok|bad>  \n",
    "grammar: <good|ok|bad>  \n",
    "tone: <good|ok|bad>  \n",
    "length: <good|ok|bad>  \n",
    "grounding: <good|ok|bad>  \n",
    "latency: <good|ok|bad>  \n",
    "cost: <good|ok|bad>  \n",
    "final_score: <good|ok|bad>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "533e9d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Batch generation helper (type‑safe) ---\n",
    "from typing import Callable\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Callable, Dict\n",
    "import pandas as pd\n",
    "\n",
    "def batch_generate(\n",
    "    sample_df: pd.DataFrame,\n",
    "    call_model_fn: Callable[[str], Dict[str, object]],\n",
    "    prompt_template: str = prompt_tmpl,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Generate descriptions and metrics for each row in *sample_df*.\n",
    "\n",
    "    The model-calling function *must* return a dict with keys:\n",
    "    - ``text`` (str) – generated description\n",
    "    \"\"\"\n",
    "    if not isinstance(sample_df, pd.DataFrame):\n",
    "        raise TypeError(\"sample_df must be a pandas DataFrame\")\n",
    "    if not callable(call_model_fn):\n",
    "        raise TypeError(\"call_model_fn must be callable\")\n",
    "\n",
    "    outputs = []\n",
    "    for _, row in sample_df.iterrows():\n",
    "        prompt = prompt_template.format(**row.to_dict())\n",
    "        out = call_model_fn(prompt)\n",
    "        if not isinstance(out, dict) or 'text' not in out:\n",
    "            raise ValueError(\"call_model_fn must return a dict with at least a 'text' field\")\n",
    "        outputs.append(out)\n",
    "\n",
    "    result_df = sample_df.copy()\n",
    "    result_df[\"generated_description\"] = [o[\"text\"] for o in outputs]\n",
    "    return result_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e16a806d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation sheet → assignment_03_jlm_sheet.xlsx with 5 rows\n"
     ]
    }
   ],
   "source": [
    "jlm_df = batch_generate(outputs_df.head(), call_gemini, prompt_template=judge_prompt_tmpl)  # NOTE: change model function as needed\n",
    "\n",
    "xlsx_path = \"assignment_03_jlm_sheet.xlsx\"\n",
    "\n",
    "jlm_df.to_excel(xlsx_path, index=False)\n",
    "print(f\"Saved evaluation sheet → {xlsx_path} with {len(outputs_df)} rows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
