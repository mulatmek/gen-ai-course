{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da9f1c7",
   "metadata": {
    "id": "8da9f1c7"
   },
   "source": [
    "# AssignmentÂ 2 â€“ Evaluating LLM Output Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac7b94",
   "metadata": {
    "id": "a0ac7b94"
   },
   "source": [
    "# Introduction to Large Language Models and Prompt Engineering\n",
    "\n",
    "**Course:** GenAI development and LLM applications\n",
    "\n",
    "\n",
    "**Instructors:** Ori Shapira, Yuval Belfer\n",
    "\n",
    "**Semester:** Summer\n",
    "    \n",
    "## Overview\n",
    "\n",
    "This assignment provides a **handsâ€‘on** experince with the world of LLM based systems evaluation: from understanding the business use-case and defining evaluation criterias in light of it. To performing human evaluation and dealing with the hardships of \"non-objectivity\", trough experimenting with **JLMs** (Judge Language Models).\n",
    "\n",
    "Along the way you will explore the differnces between the two evaluation methods, thier advanteges and dis-advanteges and try to figure out how and when to use each further down your GenAI road.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- **Define evaluation criteria** understand the importance of defining how to measure your system performance in a non closely defined problem.\n",
    "- **Compare** manual vs. automatic common methods.\n",
    "- **Drive improvement** through proper evaluation, documentation and change cycles.\n",
    "- **Design** usable automatic evaluation pipeline.\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Familiarity with Jupyter notebooks\n",
    "- Internet connection for API calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df8bfc",
   "metadata": {
    "id": "23df8bfc"
   },
   "source": [
    "# Part 1 - Human evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0637f95c",
   "metadata": {
    "id": "0637f95c"
   },
   "source": [
    "## 1â€¯Â Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1cafeb",
   "metadata": {
    "executionInfo": {
     "elapsed": 147153,
     "status": "ok",
     "timestamp": 1754149296311,
     "user": {
      "displayName": "Kim Chen",
      "userId": "03693798735466240238"
     },
     "user_tz": -180
    },
    "id": "3d1cafeb"
   },
   "outputs": [],
   "source": [
    "!pip -q install --upgrade \"transformers[torch]\" datasets accelerate bitsandbytes --progress-bar off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09b1f31",
   "metadata": {
    "id": "b09b1f31"
   },
   "source": [
    "## 2â€¯Â Business use case â€“ Generate Product Descriptions\n",
    "Many eâ€‘commerce sites need engaging **product descriptions**. Given structured attributes (name, category, features, color, price), your model should craft a persuasive, 50â€‘90â€‘word description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94d31015",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "executionInfo": {
     "elapsed": 2920,
     "status": "error",
     "timestamp": 1754149322674,
     "user": {
      "displayName": "Kim Chen",
      "userId": "03693798735466240238"
     },
     "user_tz": -180
    },
    "id": "94d31015",
    "outputId": "76060c37-b672-4511-df86-77f4a6e59ead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 products\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>Product_attribute_list</th>\n",
       "      <th>material</th>\n",
       "      <th>warranty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple iPhone 15 Pro</td>\n",
       "      <td>features: A17 Pro chip, 120â€¯Hz ProMotion displ...</td>\n",
       "      <td>titanium frame, Ceramic Shield glass</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samsung Galaxy S24 Ultra</td>\n",
       "      <td>features: 200â€¯MP camera, Sâ€‘Pen support, 120â€¯Hz...</td>\n",
       "      <td>Armor Aluminum frame, Gorilla Glass Victus</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Pixel 8 Pro</td>\n",
       "      <td>features: Tensor G3 chip, Magic Eraser, 50â€¯MP ...</td>\n",
       "      <td>matte glass back, aluminum frame</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sony WHâ€‘1000XM5 Headphones</td>\n",
       "      <td>features: active noise cancelling, 30â€¯hr batte...</td>\n",
       "      <td>synthetic leather earcups</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bose QuietComfort Ultra Earbuds</td>\n",
       "      <td>features: CustomTune sound calibration, ANC, I...</td>\n",
       "      <td>silicone ear tips</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      product_name  \\\n",
       "0              Apple iPhone 15 Pro   \n",
       "1         Samsung Galaxy S24 Ultra   \n",
       "2               Google Pixel 8 Pro   \n",
       "3       Sony WHâ€‘1000XM5 Headphones   \n",
       "4  Bose QuietComfort Ultra Earbuds   \n",
       "\n",
       "                              Product_attribute_list  \\\n",
       "0  features: A17 Pro chip, 120â€¯Hz ProMotion displ...   \n",
       "1  features: 200â€¯MP camera, Sâ€‘Pen support, 120â€¯Hz...   \n",
       "2  features: Tensor G3 chip, Magic Eraser, 50â€¯MP ...   \n",
       "3  features: active noise cancelling, 30â€¯hr batte...   \n",
       "4  features: CustomTune sound calibration, ANC, I...   \n",
       "\n",
       "                                     material                 warranty  \n",
       "0        titanium frame, Ceramic Shield glass  1â€‘year limited warranty  \n",
       "1  Armor Aluminum frame, Gorilla Glass Victus  1â€‘year limited warranty  \n",
       "2            matte glass back, aluminum frame  1â€‘year limited warranty  \n",
       "3                   synthetic leather earcups  1â€‘year limited warranty  \n",
       "4                           silicone ear tips  1â€‘year limited warranty  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the product dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"csv_files/Assignment_02_product_dataset.csv\"  # ensure the file is uploaded\n",
    "df_products = pd.read_csv(dataset_path)\n",
    "print(f\"Loaded {len(df_products)} products\")\n",
    "df_products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0fb841",
   "metadata": {
    "id": "3f0fb841"
   },
   "source": [
    "## 3â€¯Â Evaluation criteria\n",
    "| Criterion | Description | Rating |\n",
    "|-----------|-------------|--------|\n",
    "| **Fluency** | Natural, easyâ€‘toâ€‘read sentences | good / ok / bad |\n",
    "| **Grammar** | Correct spelling & punctuation | good / ok / bad |\n",
    "| **Tone** | Matches friendly, credible sales voice | good / ok / bad |\n",
    "| **Length** | 50â€‘90 words | good / ok / bad |\n",
    "| **Grounding** | Sticks to provided attributes only | good / ok / bad |\n",
    "| **Latency** | Time to first byte / full response | good / ok / bad  (based on avg. time per call)|\n",
    "| **Cost** | Relative inference or API cost per 1K tokens | good / ok / bad (based on avg. price per cal)|\n",
    "\n",
    "**Define your rubric:**\n",
    "1. For each criterion, spell out what qualifies as **good**, **ok**, and **bad** to minimize subjectivity (e.g. for *Length*: goodâ€¯=â€¯50â€‘90â€¯words, okâ€¯=â€¯40â€‘49â€¯orâ€¯91â€‘110â€¯words, badâ€¯=â€¯outside that range).\n",
    "2. Decide the **cumulative pass bar**â€”for instance, at least three *good* ratings and no *bad* ratings overall.\n",
    "3. Establish **go / noâ€‘go rules**â€”e.g. if *Grounding* is *bad* the description is automatically rejected, regardless of other scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8534af",
   "metadata": {
    "id": "ff8534af"
   },
   "source": [
    "## 4â€¯Â Prompt\n",
    "\n",
    "ðŸ’¡ **Promptâ€‘engineering tip:**\n",
    "Feel free to iterate on the prompt to maximize output quality. You can:\n",
    "- Add a **system message** that defines writing style, brand voice, or formatting rules.\n",
    "- Provide **one or two highâ€‘quality examples** (fewâ€‘shot) of attributeâ†’description pairs.\n",
    "- Include explicit constraints (word count, tone adjectives, banned phrases).\n",
    "- Experiment with phrases like *\"Think stepâ€‘byâ€‘step\"* or *\"First reason, then answer\"*.\n",
    "\n",
    "Document any changes you make and observe how they influence the evaluation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc0da72b",
   "metadata": {
    "id": "bc0da72b"
   },
   "outputs": [],
   "source": [
    "prompt_tmpl = (\n",
    "    \"You are a copywriter for an online store. Using the product attributes, \"\n",
    "    \"write an engaging product description (50â€“90 words).\\n\\n\"\n",
    "    \"Product name: {product_name}\\nFeatures: {Product_attribute_list}\\nMaterial: {material}\\nWarranty: {warranty}\\n\\n\"\n",
    "    \"Description:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b396031",
   "metadata": {
    "id": "9b396031"
   },
   "source": [
    "## 5â€¯Â Run a mediumâ€‘size model (â‰¤â€¯30â€¯B parameters)\n",
    "\n",
    "Choose **one or more** of the options below:\n",
    "\n",
    "**A.Â HuggingÂ Face checkpoint** (local inference) â€“ already configured in the code cell that follows.\n",
    "\n",
    "**B.Â OpenAI model** â€“ call an OpenAI hosted model (e.g. `gptâ€‘4o`, `gptâ€‘4â€‘turbo`, `gptâ€‘3.5â€‘turbo`). Implement `call_openai(prompt: str) -> str` in a separate utility cell and then run the snippet.\n",
    "\n",
    "**C.Â Google Gemini model** â€“ call a Gemini endpoint (e.g. `geminiâ€‘1.5â€‘pro`). Implement `call_gemini(prompt: str) -> str` similarly.\n",
    "\n",
    "> âš ï¸â€¯Make sure you have your API keys set as environment variables or passed securely.\n",
    "\n",
    "\n",
    "**Latency & cost tracking**\n",
    "- Your `call_*` functions should return a **dict** with keys:\n",
    "  - `text` â€“ generated description (string)\n",
    "  - `latency_ms` â€“ endâ€‘toâ€‘end generation time in milliseconds\n",
    "  - `input_tokens` â€“ tokens sent to the model (**IF YOU ADDED A SYS PROMPT ADD IT TO THE CALCULATION**)\n",
    "  - `output_tokens` â€“ tokens received from the model\n",
    "- Below, a helper `call_hf()` shows how to compute these metrics for a Hugging Face model. You must add equivalent tracking inside `call_openai()` and `call_gemini()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e10504",
   "metadata": {
    "id": "65e10504"
   },
   "source": [
    "**FOR COLAB USERS**\n",
    "\n",
    "You can set your HF_TOKEN secret in Colab, please follow these steps:\n",
    "\n",
    "1. Click on the \"ðŸ”‘\" icon in the left-hand sidebar in Colab. This opens the Secrets manager.\n",
    "2. Click on \"New secret\".\n",
    "3. In the \"Name\" field, type HF_TOKEN.\n",
    "4. In the \"Value\" field, paste your Hugging Face access token (you can generate one from your Hugging Face account settings under \"Access Tokens\").\n",
    "5. Make sure the \"Notebook access\" toggle is enabled for your notebook.\n",
    "6. Close the Secrets manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b468a65",
   "metadata": {
    "id": "0b468a65"
   },
   "outputs": [],
   "source": [
    "# --- OptionÂ B: OpenAI ---\n",
    "# Ensure you implemented call_openai() in another cell (you can use the one from the previous assignment).\n",
    "# from your_utils import call_openai  # example import if needed\n",
    "# response_oai = call_openai(prompt, model_name=\"gpt-4o\")\n",
    "# print(response_oai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e8efaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "307bd09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access variables\n",
    "gemini_api_key = os.getenv(\"gemini_api_key\")\n",
    "openai_api_key = os.getenv(\"openai_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3889b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "def call_openai(prompt: str,\n",
    "                model_name: str = \"chatgpt-4o-latest\",\n",
    "                api_key: str = openai_api_key) -> dict:\n",
    "    \"\"\"\n",
    "    Call OpenAI API (>=1.0) and return text + usage.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=api_key) if api_key else OpenAI()\n",
    "\n",
    "    start = time.time()\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        latency_ms = (time.time() - start) * 1000.0\n",
    "\n",
    "        text = resp.choices[0].message.content or \"\"\n",
    "        usage = getattr(resp, \"usage\", None)\n",
    "\n",
    "        return {\n",
    "            \"text\": text.strip(),\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"input_tokens\": getattr(usage, \"input_tokens\", None),\n",
    "            \"output_tokens\": getattr(usage, \"output_tokens\", None),\n",
    "            \"total_tokens\": getattr(usage, \"total_tokens\", None),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API Error: {e}\")\n",
    "        return {\n",
    "            \"text\": \"\",\n",
    "            \"latency_ms\": 0,\n",
    "            \"input_tokens\": 0,\n",
    "            \"output_tokens\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0152122a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0db18322",
   "metadata": {
    "id": "0db18322"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- OptionÂ C: Gemini ---\n",
    "def call_gemini(prompt: str, model_name: str = \"models/gemini-1.5-pro\", api_key: str = gemini_api_key) -> dict:\n",
    "    \"\"\"\n",
    "    Call Google Gemini API and return generated text and usage metrics.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user prompt.\n",
    "        model_name (str): Gemini model to use (default: \"gemini-pro\").\n",
    "        api_key (str): Google API Key.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"text\": generated description (str),\n",
    "            \"latency_ms\": latency in milliseconds (float),\n",
    "            \"input_tokens\": tokens sent (int),\n",
    "            \"output_tokens\": tokens received (int)\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    if not api_key:\n",
    "        raise ValueError(\"api_key must be provided for Google Gemini API\")\n",
    "\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "    # Initialize model\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "\n",
    "    # Measure start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        end_time = time.time()\n",
    "        latency_ms = (end_time - start_time) * 1000\n",
    "\n",
    "        text = response.text.strip() if hasattr(response, \"text\") else \"\"\n",
    "\n",
    "        # Token accounting (Gemini SDK does not expose usage directly yet)\n",
    "        input_tokens = len(prompt.split())\n",
    "        output_tokens = len(text.split())\n",
    "\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini API Error: {e}\")\n",
    "        return {\n",
    "            \"text\": \"\",\n",
    "            \"latency_ms\": 0,\n",
    "            \"input_tokens\": 0,\n",
    "            \"output_tokens\": 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "554467dd",
   "metadata": {
    "id": "554467dd"
   },
   "outputs": [],
   "source": [
    "# --- Batch generation helper (typeâ€‘safe) ---\n",
    "from typing import Callable\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Callable, Dict\n",
    "import pandas as pd\n",
    "\n",
    "def batch_generate(\n",
    "    sample_df: pd.DataFrame,\n",
    "    call_model_fn: Callable[[str], Dict[str, object]],\n",
    "    prompt_template: str = prompt_tmpl,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Generate descriptions and metrics for each row in *sample_df*.\n",
    "\n",
    "    The model-calling function *must* return a dict with keys:\n",
    "    - ``text`` (str) â€“ generated description\n",
    "    - ``latency_ms`` (float | None)\n",
    "    - ``input_tokens`` (int | None)\n",
    "    - ``output_tokens`` (int | None)\n",
    "    \"\"\"\n",
    "    if not isinstance(sample_df, pd.DataFrame):\n",
    "        raise TypeError(\"sample_df must be a pandas DataFrame\")\n",
    "    if not callable(call_model_fn):\n",
    "        raise TypeError(\"call_model_fn must be callable\")\n",
    "\n",
    "    outputs = []\n",
    "    for _, row in sample_df.iterrows():\n",
    "        prompt = prompt_template.format(**row.to_dict())\n",
    "        out = call_model_fn(prompt)\n",
    "        if not isinstance(out, dict) or 'text' not in out:\n",
    "            raise ValueError(\"call_model_fn must return a dict with at least a 'text' field\")\n",
    "        outputs.append(out)\n",
    "\n",
    "    result_df = sample_df.copy()\n",
    "    result_df[\"generated_description\"] = [o[\"text\"] for o in outputs]\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "demo_df = batch_generate(df_products[:5], call_openai)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25cb4ba",
   "metadata": {
    "id": "e25cb4ba"
   },
   "source": [
    "## 6â€¯Â Manual evaluation\n",
    "Use `batch_generate()` to create a DataFrame of model outputs, then add blank rating columns for each criterion plus a `final_score` column. An Excel file is saved so you can fill scores offline or share with peers.\n",
    "\n",
    "Steps:\n",
    "1. Run the code cell below (adjust which `call_*` function you pass in).\n",
    "2. Open the generated `assignment_03_evaluation_sheet.xlsx` and rate each row with **good / ok / bad**.\n",
    "3. Add a rule for `final_score` (e.g., majority = good, fails if grounding = bad).\n",
    "\n",
    "\n",
    "**Cost calculator**\n",
    "Use the helper below to compute cost in USD based on token usage:\n",
    "```python\n",
    "outputs_df = add_cost_columns(outputs_df, input_price_per_m=1.5, output_price_per_m=2.0)\n",
    "```\n",
    "Set prices to **0** if you ran everything locally on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4386c3f5",
   "metadata": {
    "id": "4386c3f5"
   },
   "outputs": [],
   "source": [
    "# --- Build evaluation sheet & export to Excel ---\n",
    "\n",
    "#Update the prices according to the model you used, or leave them at 0 for HF local models\n",
    "YOUR_MODEL_INPUT_PRICE_PER_M = 1.25\n",
    "YOUR_MODEL_OUTPUT_PRICE_PER_M = 5\n",
    "outputs_df = batch_generate(df_products.head(), call_openai)  # NOTE: change model function as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dcbe33",
   "metadata": {
    "id": "38dcbe33"
   },
   "source": [
    "## 7â€¯Â Improvement cycle\n",
    "\n",
    "Now that youâ€™ve established a baseline score in **Sectionâ€¯6**, iterate to achieve better results.\n",
    "\n",
    "**Ideas to explore**\n",
    "- **Prompt tuning**Â â€“ rewrite the system/user prompts, add fewâ€‘shot examples, or enforce stricter constraints.\n",
    "- **Model choice**Â â€“ test a different checkpoint (larger â‰  always better), switch from HF to OpenAI or Gemini, or try a domainâ€‘specific model.\n",
    "- **Temperature / decoding params**Â â€“ adjust `temperature`, `top_p`, `top_k`, or `max_new_tokens` to balance creativity vs. factuality.\n",
    "- **Data preprocessing**Â â€“ clean attribute text, expand abbreviations, or group similar products to feed additional context.\n",
    "- **Postâ€‘processing**Â â€“ run grammarâ€‘checking or length trimming after generation.\n",
    "- **Ensembling / RAG**Â â€“ combine outputs from two models or ground the prompt with retrieved copy from existing catalog listings.\n",
    "\n",
    "Document each experiment in a brief bullet list:\n",
    "1. **What you changed**\n",
    "2. **Why you expected it to help**\n",
    "3. **New evaluation scores**\n",
    "\n",
    "ðŸ’¡ *Goal*: maximize the cumulative score according to your rubric while respecting the go/noâ€‘go rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0534c68b",
   "metadata": {
    "id": "0534c68b"
   },
   "source": [
    "# Partâ€¯2Â â€“ JudgingÂ LanguageÂ ModelsÂ (JLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d584e",
   "metadata": {
    "id": "045d584e"
   },
   "source": [
    "In PartÂ 1 we generated model outputs and evaluated the outputs according to a set of business criterion.\n",
    "Now we will build **JLMs** that (try really hard to match what you did and) automatically *grade* those outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ff34e2",
   "metadata": {
    "id": "d5ff34e2"
   },
   "source": [
    "## 1Â NaÃ¯veÂ JLMÂ (same model as generation)\n",
    "\n",
    "For the productâ€‘description task there is **no single reference answer**.  \n",
    "Your naÃ¯ve JLM should simply *critique the description itself* using the same\n",
    "LLM that produced it.\n",
    "\n",
    "* Inputs: the generated product description (string) and any rubric bullets you\n",
    "  think matter (e.g. factual accuracy, persuasiveness, tone).  \n",
    "* Output: **any useful signal** â€“ freeâ€‘text comments, a JSON blob, a (score, notes) tupleâ€¦ your choice.\n",
    "\n",
    "> **Deliverable:** implement `naive_jlm(description)` that calls the LLM once and\n",
    "> returns its raw judgment output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "46657a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompt_tmpl =\"\"\"\n",
    "You are an expert evaluator of marketing copy. Based on the product information and the generated description below, evaluate the description using **only** these labels: \"good\", \"ok\", or \"bad\".\n",
    "\n",
    "Rate each of the following categories:\n",
    "- fluency\n",
    "- grammar\n",
    "- tone\n",
    "- length (should be between 50â€“90 words)\n",
    "- grounding (faithfulness to product attributes)\n",
    "- latency (perceived delivery speed / verbosity)\n",
    "- cost (efficiency of wording)\n",
    "- final_score (overall quality)\n",
    "\n",
    "**Product Details**\n",
    "- Product name: {product_name}\n",
    "- Key features: {Product_attribute_list}\n",
    "- Material: {material}\n",
    "- Warranty: {warranty}\n",
    "\n",
    "**Generated Description**\n",
    "{generated_description}\n",
    "\n",
    "Return your evaluation in the following JSON format:\n",
    "\n",
    "{{\n",
    "  \"fluency\": \"<good|ok|bad>\",\n",
    "  \"grammar\": \"<good|ok|bad>\",\n",
    "  \"tone\": \"<good|ok|bad>\",\n",
    "  \"length\": \"<good|ok|bad>\",\n",
    "  \"grounding\": \"<good|ok|bad>\",\n",
    "  \"latency\": \"<good|ok|bad>\",\n",
    "  \"cost\": \"<good|ok|bad>\",\n",
    "  \"final_score\": \"<good|ok|bad>\"\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f3d6fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"Experience the pinnacle of innovation with the Apple iPhone 15 Pro.  Its powerful A17 Pro chip fuels a breathtaking 120Hz ProMotion display for incredibly smooth visuals.  The durable titanium frame and Ceramic Shield glass encase this compact powerhouse. Enjoy blazing-fast charging with USB-C. Comes with a 1-year limited warranty for peace of mind. Upgrade to pro.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6725212c",
   "metadata": {
    "id": "6725212c"
   },
   "outputs": [],
   "source": [
    "# ------------------------- STUDENT TODO -------------------------\n",
    "# Implement a simple selfâ€‘critique using the SAME model used for generation.\n",
    "# No reference answer is available, so evaluate the description directly.\n",
    "\n",
    "# IMPORTANT NOTE :use the same model as the one you used for generation\n",
    "def naive_jlm(description: str):\n",
    "    \"\"\"Return a naive judgment (format of your choice) for a product description.\"\"\"\n",
    "    # Example pseudocode:\n",
    "    prompt = \"\"\"You are a productâ€‘description critic. Evaluate the following description\n",
    "     on clarity, appeal, and accuracy. Return JSON with 'strengths', 'weaknesses',\n",
    "     and an overall 'rating' from 1â€‘5.\\n\\nDescription:\\n{description}\\nJSON:\"\"\"\n",
    "    \n",
    "    response = call_openai(prompt.format(description=description))[\"text\"]\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "15c09aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_out = naive_jlm(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ad7740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bba93214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'strengths': ['Clear mention of key features such as the A17 Pro chip, 120Hz ProMotion display, and USB-C charging',\n",
       "  \"Appealing language like 'pinnacle of innovation' and 'compact powerhouse' enhances product desirability\",\n",
       "  'Brief but informativeâ€”covers hardware, performance, durability, and warranty',\n",
       "  'Mentions the premium build materials (titanium frame, Ceramic Shield), which adds credibility and luxury appeal'],\n",
       " 'weaknesses': ['Lacks specific technical details such as screen size, battery life, or camera specifications, which could help with informed decision-making',\n",
       "  \"Phrases like 'blazing-fast charging' are vague and could be supported with numbers (e.g., wattage or charging time)\",\n",
       "  \"The phrase 'Upgrade to pro' is somewhat generic and doesnâ€™t add meaningful information\"],\n",
       " 'rating': 4}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(naive_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a9eb1",
   "metadata": {
    "id": "466a9eb1"
   },
   "source": [
    "## 2Â StructuredÂ JLMÂ â€“ JSON per criterion\n",
    "\n",
    "We want our grader to return **machineâ€‘readable JSON** so that we can later **automate\n",
    "aggregate scoring and analysis** (e.g., Sectionâ€¯5 majority vote, dashboards, or batch\n",
    "experiments).\n",
    "\n",
    "For every evaluation criterion you defined in PartÂ 1Â sectoin 3, output an object that looks\n",
    "like this (keys must match your own criteria list):\n",
    "\n",
    "* **Fluency**  \n",
    "* **Grammar**  \n",
    "* **Tone**  \n",
    "* **Length**  \n",
    "* **Grounding**Â Â (e.g. factual alignment / no hallucinations)\n",
    "\n",
    "We will grade every description on the **criteria** we care about for this\n",
    "assignment:\n",
    "```json\n",
    "{\n",
    "  \"Fluency\":   {\"explanation\": \"...\", \"verdict\": \"good\"},\n",
    "  \"Grammar\":   {\"explanation\": \"...\", \"verdict\": \"ok\"},\n",
    "  \"Tone\":      {\"explanation\": \"...\", \"verdict\": \"good\"},\n",
    "  \"Length\":    {\"explanation\": \"...\", \"verdict\": \"good\"},\n",
    "  \"Grounding\": {\"explanation\": \"...\", \"verdict\": \"good\"},\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "*\\*The following will be ignored for convinence and as they do not require JLM*\n",
    "* **Latency**\n",
    "* **Cost**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Why `{explanation, verdict}`?\n",
    "\n",
    "1. **Justification before decision** â€“ forcing the model to produce an *explanation*\n",
    "   **first** encourages reasoned thinking; if it wrote the verdict first, the\n",
    "   explanation might simply try to defend that label (confirmation bias).  \n",
    "2. **Auditability** â€“ having a short explanation lets us *spotâ€‘check* the grader\n",
    "   for hallucinations or misâ€‘interpretations down the line.  \n",
    "3. **Closedâ€‘class verdicts** â€“ restricting the verdict to a small set\n",
    "   (`goodâ€¯|â€¯okâ€¯|â€¯bad`) enables fast, deterministic score calculations in later\n",
    "   automated pipelines.\n",
    "\n",
    "### Your implementation\n",
    "\n",
    "* **Backend choice:** `call_openai()`, `call_gemini()`, `call_hf()`, or a\n",
    "  LangChain chat model â€“ choose whatever you prefer.  \n",
    "* **Prompt freedom:** use the template we provide *or rewrite it entirely*.  \n",
    "* Loop over the global `criteria` list from PartÂ 1Â Â§3.  \n",
    "* Implement **`structured_jlm(description)`** so the cell runs without errors and\n",
    "  returns the parsed JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "adf819d3",
   "metadata": {
    "id": "adf819d3"
   },
   "outputs": [],
   "source": [
    "# ------------------------- STUDENT TODO -------------------------\n",
    "import json\n",
    "\n",
    "# Fixed criteria list for the assignment\n",
    "criteria = [\"Fluency\", \"Grammar\", \"Tone\", \"Length\", \"Grounding\"]\n",
    "\n",
    "# Choose backend: \"openai\", \"gemini\", \"hf\"\n",
    "BACKEND = \"openai\"\n",
    "\n",
    "# Prompt template (edit freely)\n",
    "PROMPT_TEMPLATE = \"\"\"You are a productâ€‘description critic.\n",
    "For each criterion in the list below, provide a JSON object with:\n",
    "  â€¢ explanation (1â€‘2 sentences)\n",
    "  â€¢ verdict      (good | ok | bad)\n",
    "\n",
    "Criteria: {criteria}\n",
    "\n",
    "Description:\n",
    "{description}\n",
    "\n",
    "JSON:\"\"\"\n",
    "\n",
    "#IMPORTANT NOTE :use the same model as the one you used for generation\n",
    "def call_model(prompt: str, model_name: str) -> str:\n",
    "    if BACKEND == \"openai\":\n",
    "        return call_openai(prompt, model_name)[\"text\"]\n",
    "    elif BACKEND == \"gemini\":\n",
    "        return call_gemini(prompt, model_name)\n",
    "    elif BACKEND == \"hf\":\n",
    "        return call_hf(prompt, model_name)\n",
    "\n",
    "def structured_jlm(description: str, model_name:str) -> dict:\n",
    "    prompt = PROMPT_TEMPLATE.format(criteria=\", \".join(criteria),\n",
    "                                    description=description)\n",
    "    raw = call_model(prompt, model_name) # NOTICE this is the raw response, modify it as you will to make the next step work\n",
    "\n",
    "    try: #NOTE: Making sure your model returns a valid JSON object is sometimes harder than you think...\n",
    "        return json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(f\"Error: Invalid JSON response from model: {raw}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "structured_out = structured_jlm(description, model_name=\"chatgpt-4o-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0579206c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fluency': {'explanation': 'The description reads smoothly and logically, with a coherent flow between features and benefits.',\n",
       "  'verdict': 'good'},\n",
       " 'Grammar': {'explanation': 'There are no grammatical errors; punctuation and sentence structure are correct throughout.',\n",
       "  'verdict': 'good'},\n",
       " 'Tone': {'explanation': 'The tone is enthusiastic and promotional, well-suited for a high-end tech product like the iPhone 15 Pro.',\n",
       "  'verdict': 'good'},\n",
       " 'Length': {'explanation': 'The description is concise while covering key features, making it well-suited for online retail or marketing contexts.',\n",
       "  'verdict': 'good'},\n",
       " 'Grounding': {'explanation': 'All claims are consistent with known specifications of the iPhone 15 Pro, such as the A17 Pro chip and USB-C charging.',\n",
       "  'verdict': 'good'}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb2b21",
   "metadata": {
    "id": "92fb2b21"
   },
   "source": [
    "## 3Â Criterionâ€‘byâ€‘CriterionÂ Calls\n",
    "\n",
    "Instead of asking the JLM to grade *all* criteria in one big prompt, we can\n",
    "make **one call per criterion** and merge the results.\n",
    "\n",
    "### Why split?\n",
    "\n",
    "1. **Sharper focusÂ â‡¢ better judgments**  \n",
    "   The model concentrates on a *single* dimension at a time, which often boosts\n",
    "   accuracy â€“ atomic tasks are easier.  \n",
    "2. **Richer, criterionâ€‘specific instructions**  \n",
    "   You can craft a longer, more detailed prompt for *each* criterion without\n",
    "   squeezing everything into one context window.\n",
    "\n",
    "### Tradeâ€‘offs\n",
    "\n",
    "1. **Higher cost** â€“ `#criteria Ã— priceâ€‘perâ€‘call` can add up.  \n",
    "2. **Higher latency** â€“ multiple sequential calls take longer than one.\n",
    "\n",
    "\n",
    "\n",
    "### Your task\n",
    "\n",
    "1. Implement **`per_criterion_jlm(description)`** (see code cell below) using any backend (`call_openai`, `call_gemini`, `call_hf`, or `jlm_llm`).\n",
    "2. **Pick one generated product description** and run **all three graders**:\n",
    "   * `naive_jlm(description)` (SectionÂ 1)\n",
    "   * `structured_jlm(description)` (SectionÂ 2)\n",
    "   * `per_criterion_jlm(description)` (this section)\n",
    "3. Compare the outputs. Note any differences in explanations and verdicts.\n",
    "\n",
    "Write twoâ€‘three sentences of reflection in the markdown cell that follows: *Which approach seems most trustworthy for this task and why?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a75867ae",
   "metadata": {
    "id": "a75867ae"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "criteria = [\"Fluency\", \"Grammar\", \"Tone\", \"Length\", \"Grounding\"]\n",
    "\n",
    "# Reâ€‘use BACKEND and call_model() from SectionÂ 2, or redefine here.\n",
    "try:\n",
    "    BACKEND\n",
    "except NameError:\n",
    "    BACKEND = \"hf\"  # fallback; set to openai / gemini / hf as needed\n",
    "\n",
    "# If call_model is not defined (e.g., you skipped Sectionâ€¯2), define a minimal version\n",
    "if \"call_model\" not in globals():\n",
    "    def call_model(prompt: str, model_name: str) -> str:\n",
    "        if BACKEND == \"openai\":\n",
    "            return call_openai(prompt, model_name)\n",
    "        elif BACKEND == \"gemini\":\n",
    "            return call_gemini(prompt, model_name)\n",
    "        elif BACKEND == \"hf\":\n",
    "            return call_hf(prompt, model_name)\n",
    "\n",
    "# Template per criterion (modify as you like)\n",
    "CRIT_PROMPT_TEMPLATE = \"\"\"You are a productâ€‘description critic.\n",
    "Criterion: {criterion}\n",
    "Return JSON with keys 'explanation' and 'verdict' (good|ok|bad).\n",
    "\n",
    "Description:\n",
    "{description}\n",
    "\n",
    "JSON:\"\"\"\n",
    "\n",
    "#IMPORTANT NOTE :use the same model as the one you used for generation\n",
    "def per_criterion_jlm(description: str, model_name: str, criteria: list[str] | None = None) -> dict:\n",
    "    \"\"\"Grade *description* against each criterion in *criteria* with separate LLM calls.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    description : str\n",
    "        Product description to evaluate.\n",
    "    criteria : list[str], optional\n",
    "        List of evaluation criteria. If *None*, falls back to the global ``criteria`` list\n",
    "        defined in this notebook.\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping from criterion name to the parsed JSON returned by the judgeâ€‘LLM.\n",
    "    \"\"\"\n",
    "    if criteria is None:\n",
    "        # Use the global variable if available; otherwise default to common criteria.\n",
    "        criteria = globals().get(\"criteria\", [\"Fluency\", \"Grammar\", \"Tone\", \"Length\", \"Grounding\"])\n",
    "\n",
    "    results = {}\n",
    "    for crit in criteria:\n",
    "        prompt = CRIT_PROMPT_TEMPLATE.format(criterion=crit, description=description)\n",
    "        raw = call_model(prompt, model_name)\n",
    "        try:\n",
    "            results[crit] = json.loads(raw)\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(f\"Error parsing JSON for criterion {crit}: {raw}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1730052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = per_criterion_jlm(description,\"chatgpt-4o-latest\", criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a526f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fluency': {'explanation': 'The product description is fluent and well-structured, with clear and grammatically correct sentences. It uses varied sentence structures and transitions smoothly between features. The vocabulary is appropriate and engaging, enhancing the overall readability and promotional tone.',\n",
       "  'verdict': 'good'},\n",
       " 'Grammar': {'explanation': \"The grammar in the product description is strong overall. Sentences are well-structured, punctuation is used correctly, and there are no spelling or subject-verb agreement issues. Minor style improvements could be made, such as avoiding sentence fragments like 'Comes with a 1-year limited warranty for peace of mind,' which lacks a clear subject. However, this is acceptable in marketing copy.\",\n",
       "  'verdict': 'good'},\n",
       " 'Tone': {'explanation': \"The tone of the product description is confident, sleek, and aspirationalâ€”well-suited for a premium tech product like the iPhone 15 Pro. It uses enthusiastic and descriptive language such as 'pinnacle of innovation,' 'breathtaking,' and 'compact powerhouse' to appeal to a high-end consumer. The tone is consistent and aligns with Apple's branding style, making it effective for the target audience.\",\n",
       "  'verdict': 'good'},\n",
       " 'Length': {'explanation': \"The product description is concise and informative, highlighting key features of the iPhone 15 Pro such as the A17 Pro chip, 120Hz display, titanium frame, USB-C charging, and warranty. It maintains a balance between brevity and detail, making it easy for consumers to quickly grasp the product's benefits without being overwhelmed.\",\n",
       "  'verdict': 'good'},\n",
       " 'Grounding': {'explanation': 'The description references specific and verifiable features of the Apple iPhone 15 Pro, such as the A17 Pro chip, 120Hz ProMotion display, titanium frame, Ceramic Shield glass, USB-C charging, and a 1-year limited warranty. All these elements are grounded in actual product specifications provided by Apple, indicating that the claims are based on factual information.',\n",
       "  'verdict': 'good'}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318fec16",
   "metadata": {},
   "source": [
    "The per-criterion grader felt most trustworthy: judging one dimension at a time produced clearer, actionable explanations and caught nuances (e.g., Length/Grounding) that the single-shot graders glossed over. The structured grader was solid but noticeably more lenient, while the naive grader tended to be generous and less specific. Despite the extra calls (cost/latency), the per-criterion approach gives better transparency and debuggability for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca557369",
   "metadata": {
    "id": "ca557369"
   },
   "source": [
    "## 4Â DifferentÂ ModelÂ JLM\n",
    "\n",
    "### Why switch models?\n",
    "\n",
    "We **do not reuse the generator model as the judge** because:\n",
    "\n",
    "1. **Selfâ€‘evaluation bias** â€“ a model tends to justify its own outputs, consciously or not, and can overâ€‘score itself.  \n",
    "2. **Correlated failure modes** â€“ the generatorâ€™s blindâ€‘spots (hallucinations, sloppy reasoning) may also appear in its judgments.  \n",
    "3. **Diversity of signal** â€“ using an *independent* model (preferably from another provider) gives a more reliable second opinion.\n",
    "\n",
    "### Your task\n",
    "\n",
    "* Pick a **different provider** than you used in SectionsÂ 1â€‘3 (e.g. if you relied on OpenAI, try Gemini or Claude; if you used HF, try OpenAI, etc.).  \n",
    "* Implement **`per_criterion_jlm_alt()`** â€“ a thin wrapper that calls `per_criterion_jlm()` *after temporarily switching* the backend to this alternate provider, so it still returns the usual `{explanation, verdict}` JSON for **each criterion**.  \n",
    "  * You may use helpers such as `call_openai`, `call_gemini`, `call_hf`, or any LangChain chat modelâ€”your choice.  \n",
    "  * Feel free to craft a new prompt tailored to the provider.  \n",
    "\n",
    "### Compare\n",
    "\n",
    "Run both graders â€“ **SectionÂ 3â€™s `per_criterion_jlm()`** and the new **`per_criterion_jlm_alt()`** â€“ on the *same* product description:\n",
    "\n",
    "* Are the verdicts identical?  \n",
    "* Does the new JLM appear to **favour or penalise** the generated answer differently?  \n",
    "* Add a brief reflection in the markdown cell that follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "49edbab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKEND = \"gemini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "26864188",
   "metadata": {
    "id": "26864188"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "criteria = [\"Fluency\", \"Grammar\", \"Tone\", \"Length\", \"Grounding\"]\n",
    "\n",
    "# Reâ€‘use BACKEND and call_model() from SectionÂ 2, or redefine here.\n",
    "try:\n",
    "    BACKEND\n",
    "except NameError:\n",
    "    BACKEND = \"hf\"  # fallback; set to openai / gemini / hf as needed\n",
    "\n",
    "# If call_model is not defined (e.g., you skipped Sectionâ€¯2), define a minimal version\n",
    "if \"call_model\" not in globals():\n",
    "    def call_model(prompt: str, model_name: str) -> str:\n",
    "        if BACKEND == \"openai\":\n",
    "            return call_openai(prompt, model_name)\n",
    "        elif BACKEND == \"gemini\":\n",
    "            return call_gemini(prompt, model_name)\n",
    "        elif BACKEND == \"hf\":\n",
    "            return call_hf(prompt, model_name)\n",
    "\n",
    "# Template per criterion (modify as you like)\n",
    "CRIT_PROMPT_TEMPLATE = \"\"\"You are a productâ€‘description critic.\n",
    "Criterion: {criterion}\n",
    "Return JSON with keys 'explanation' and 'verdict' (good|ok|bad).\n",
    "\n",
    "Description:\n",
    "{description}\n",
    "\n",
    "JSON:\"\"\"\n",
    "\n",
    "# IMPORTANT NOTE: use the same model you used for generation\n",
    "def per_criterion_jlm(description: str, model_name: str, criteria: list[str] | None = None) -> dict:\n",
    "    \"\"\"\n",
    "    Grade *description* against each criterion in *criteria* with separate LLM calls.\n",
    "    Works with Gemini / OpenAI / HF. Robust to fenced JSON (```json ... ```).\n",
    "    \"\"\"\n",
    "    import re, json\n",
    "\n",
    "    if criteria is None:\n",
    "        criteria = globals().get(\"criteria\", [\"Fluency\", \"Grammar\", \"Tone\", \"Length\", \"Grounding\"])\n",
    "\n",
    "    # --- helpers (local so the rest of your notebook stays unchanged) ---\n",
    "    def _first_json_object(s: str) -> str | None:\n",
    "        s = str(s)\n",
    "        start = s.find(\"{\")\n",
    "        if start == -1:\n",
    "            return None\n",
    "        depth = 0\n",
    "        for i in range(start, len(s)):\n",
    "            c = s[i]\n",
    "            if c == \"{\":\n",
    "                depth += 1\n",
    "            elif c == \"}\":\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    return s[start:i+1]\n",
    "        return None\n",
    "\n",
    "    def _clean_llm_json_text(text: str) -> str:\n",
    "        \"\"\"Strip code fences and extra prose; return just the JSON object if found.\"\"\"\n",
    "        s = str(text).strip()\n",
    "        if s.startswith(\"```\"):\n",
    "            s = re.sub(r\"^```[a-zA-Z0-9_-]*\\s*\", \"\", s)\n",
    "            s = re.sub(r\"\\s*```$\", \"\", s)\n",
    "        obj = _first_json_object(s)\n",
    "        return obj if obj is not None else s\n",
    "\n",
    "    results = {}\n",
    "    for crit in criteria:\n",
    "        prompt = CRIT_PROMPT_TEMPLATE.format(criterion=crit, description=description)\n",
    "\n",
    "        # Your call_model may return dict ({\"text\": ...}) or plain string; normalize to text\n",
    "        out = call_model(prompt, model_name)\n",
    "        text = out.get(\"text\", \"\") if isinstance(out, dict) else out\n",
    "\n",
    "        # Clean up Gemini-style fenced JSON (and any extra prose) before parsing\n",
    "        text = _clean_llm_json_text(text)\n",
    "\n",
    "        try:\n",
    "            results[crit] = json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(f\"Error parsing JSON for criterion {crit}: {text}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a1bb8a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = per_criterion_jlm(description,\"gemini-1.5-pro\", criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bf573e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fluency': {'text': '```json\\n{\\n  \"explanation\": \"The description is fluent and easy to read.  The language used is appropriate for marketing an Apple product, employing persuasive vocabulary like \\\\\"pinnacle of innovation,\\\\\" \\\\\"breathtaking,\\\\\" and \\\\\"blazing-fast.\\\\\"  The sentence structure is varied, avoiding monotony.  There is a clear progression of features.\",\\n  \"verdict\": \"good\"\\n}\\n```',\n",
       "  'latency_ms': 3942.1138763427734,\n",
       "  'input_tokens': 73,\n",
       "  'output_tokens': 49},\n",
       " 'Grammar': {'text': '```json\\n{\\n  \"explanation\": \"The grammar in the product description is excellent.  All sentences are correctly structured, the punctuation is accurate, and the language is clear and concise. There are no grammatical errors.\",\\n  \"verdict\": \"good\"\\n}\\n```',\n",
       "  'latency_ms': 3587.919235229492,\n",
       "  'input_tokens': 73,\n",
       "  'output_tokens': 36},\n",
       " 'Tone': {'text': '```json\\n{\\n  \"explanation\": \"The tone is overly enthusiastic and uses marketing hype (\\'pinnacle of innovation\\', \\'breathtaking\\', \\'blazing-fast\\'). While aiming for aspirational, it comes across as slightly cheesy and unconvincing. The phrase \\'Upgrade to pro\\' is particularly jarring and feels forced.\",\\n  \"verdict\": \"ok\"\\n}\\n```',\n",
       "  'latency_ms': 4754.6868324279785,\n",
       "  'input_tokens': 73,\n",
       "  'output_tokens': 44},\n",
       " 'Length': {'text': '```json\\n{\\n  \"explanation\": \"The description is concise and avoids unnecessary jargon.  It hits the key selling points without being overly verbose.  However, it could benefit from slightly more detail in certain areas, particularly regarding camera capabilities or battery life, which are major considerations for phone buyers.\",\\n  \"verdict\": \"ok\"\\n}\\n```',\n",
       "  'latency_ms': 4039.9301052093506,\n",
       "  'input_tokens': 73,\n",
       "  'output_tokens': 50},\n",
       " 'Grounding': {'text': '```json\\n{\\n  \"explanation\": \"The description uses evocative language like \\\\\"pinnacle of innovation\\\\\" and \\\\\"breathtaking,\\\\\" but lacks concrete, measurable details to ground these claims.  While it mentions features (A17 Pro chip, 120Hz display, titanium frame, Ceramic Shield, USB-C charging), it doesn\\'t provide specifics like battery life, screen resolution, storage options, or charging speed.  These omissions prevent the reader from forming a grounded understanding of the product\\'s capabilities and value.\",\\n  \"verdict\": \"bad\"\\n}\\n```',\n",
       "  'latency_ms': 4844.1150188446045,\n",
       "  'input_tokens': 73,\n",
       "  'output_tokens': 72}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab06c1f0",
   "metadata": {
    "id": "ab06c1f0"
   },
   "source": [
    "## 5Â Creating aÂ Datasetâ€‘Evaluation Infrastructure\n",
    "\n",
    "You now have several JLM graders (`structured_jlm`, `per_criterion_jlm` and `naive_jlm`) that can judge a *single* product description.  \n",
    "The next step is to **scale that evaluation to an entire dataset** of model outputs so you can compute corpusâ€‘level metrics and quickly spot problematic cases.\n",
    "\n",
    "### Why build an infrastructure?\n",
    "\n",
    "* **ReproducibilityÂ & tracking** â€“ running all descriptions through the *same* grading pipeline lets you store the verdict/explanation alongside the raw generations for future audits.  \n",
    "* **Bulk analytics** â€“ with judgments in a DataFrame you can compute passâ€‘rates, perâ€‘criterion confusion matrices, or sliceâ€‘byâ€‘slice performance (e.g., electronics vs. clothing).  \n",
    "* **Error triage** â€“ the explanation text helps you cluster or search for recurring failure modes (e.g., \"missing dimensions\", \"overâ€‘hyping\").  \n",
    "* **Continuous evaluation** â€“ once scripted, you can rerun the notebook after each model iteration and track deltas automatically.\n",
    "\n",
    "### What to implement\n",
    "\n",
    "1. **Iterate over the dataset** â€“ a `pandas.DataFrame` where each row has at least a `description` column (and optionally metadata like `product_id`, `category`).  \n",
    "2. **Apply your chosen grader** â€“ call **one** of your JLM functions (`per_criterion_jlm` is a good default) per row.  \n",
    "3. **Add columns** â€“ for every evaluation criterion create `<criterion>_explanation` and `<criterion>_verdict` columns.  \n",
    "4. **Save or return the enriched DataFrame** so you can inspect it, compute stats, or export to CSV/Parquet.\n",
    "\n",
    "A starter helper function is provided in the code cell belowâ€”customise it as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d5800136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>Product_attribute_list</th>\n",
       "      <th>material</th>\n",
       "      <th>warranty</th>\n",
       "      <th>generated_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple iPhone 15 Pro</td>\n",
       "      <td>features: A17 Pro chip, 120â€¯Hz ProMotion displ...</td>\n",
       "      <td>titanium frame, Ceramic Shield glass</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "      <td>Unleash next-level performance with the Apple ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samsung Galaxy S24 Ultra</td>\n",
       "      <td>features: 200â€¯MP camera, Sâ€‘Pen support, 120â€¯Hz...</td>\n",
       "      <td>Armor Aluminum frame, Gorilla Glass Victus</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "      <td>Unleash the extraordinary with the Samsung Gal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Pixel 8 Pro</td>\n",
       "      <td>features: Tensor G3 chip, Magic Eraser, 50â€¯MP ...</td>\n",
       "      <td>matte glass back, aluminum frame</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "      <td>Meet the Google Pixel 8 Pro â€” the smartphone e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sony WHâ€‘1000XM5 Headphones</td>\n",
       "      <td>features: active noise cancelling, 30â€¯hr batte...</td>\n",
       "      <td>synthetic leather earcups</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "      <td>Immerse yourself in pure sound with the Sony W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bose QuietComfort Ultra Earbuds</td>\n",
       "      <td>features: CustomTune sound calibration, ANC, I...</td>\n",
       "      <td>silicone ear tips</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "      <td>Experience audio like never before with the Bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      product_name  \\\n",
       "0              Apple iPhone 15 Pro   \n",
       "1         Samsung Galaxy S24 Ultra   \n",
       "2               Google Pixel 8 Pro   \n",
       "3       Sony WHâ€‘1000XM5 Headphones   \n",
       "4  Bose QuietComfort Ultra Earbuds   \n",
       "\n",
       "                              Product_attribute_list  \\\n",
       "0  features: A17 Pro chip, 120â€¯Hz ProMotion displ...   \n",
       "1  features: 200â€¯MP camera, Sâ€‘Pen support, 120â€¯Hz...   \n",
       "2  features: Tensor G3 chip, Magic Eraser, 50â€¯MP ...   \n",
       "3  features: active noise cancelling, 30â€¯hr batte...   \n",
       "4  features: CustomTune sound calibration, ANC, I...   \n",
       "\n",
       "                                     material                 warranty  \\\n",
       "0        titanium frame, Ceramic Shield glass  1â€‘year limited warranty   \n",
       "1  Armor Aluminum frame, Gorilla Glass Victus  1â€‘year limited warranty   \n",
       "2            matte glass back, aluminum frame  1â€‘year limited warranty   \n",
       "3                   synthetic leather earcups  1â€‘year limited warranty   \n",
       "4                           silicone ear tips  1â€‘year limited warranty   \n",
       "\n",
       "                               generated_description  \n",
       "0  Unleash next-level performance with the Apple ...  \n",
       "1  Unleash the extraordinary with the Samsung Gal...  \n",
       "2  Meet the Google Pixel 8 Pro â€” the smartphone e...  \n",
       "3  Immerse yourself in pure sound with the Sony W...  \n",
       "4  Experience audio like never before with the Bo...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2efc9b9c",
   "metadata": {
    "id": "2efc9b9c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>Product_attribute_list</th>\n",
       "      <th>material</th>\n",
       "      <th>warranty</th>\n",
       "      <th>generated_description</th>\n",
       "      <th>Fluency_explanation</th>\n",
       "      <th>Fluency_verdict</th>\n",
       "      <th>Grammar_explanation</th>\n",
       "      <th>Grammar_verdict</th>\n",
       "      <th>Tone_explanation</th>\n",
       "      <th>Tone_verdict</th>\n",
       "      <th>Length_explanation</th>\n",
       "      <th>Length_verdict</th>\n",
       "      <th>Grounding_explanation</th>\n",
       "      <th>Grounding_verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple iPhone 15 Pro</td>\n",
       "      <td>features: A17 Pro chip, 120â€¯Hz ProMotion displ...</td>\n",
       "      <td>titanium frame, Ceramic Shield glass</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "      <td>Unleash next-level performance with the Apple ...</td>\n",
       "      <td>The description flows well and is easy to unde...</td>\n",
       "      <td>good</td>\n",
       "      <td>The grammar in this description is excellent. ...</td>\n",
       "      <td>good</td>\n",
       "      <td>The tone is professional and enthusiastic, hig...</td>\n",
       "      <td>good</td>\n",
       "      <td>This description is a good length. It highligh...</td>\n",
       "      <td>good</td>\n",
       "      <td>The description provides specific details that...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samsung Galaxy S24 Ultra</td>\n",
       "      <td>features: 200â€¯MP camera, Sâ€‘Pen support, 120â€¯Hz...</td>\n",
       "      <td>Armor Aluminum frame, Gorilla Glass Victus</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "      <td>Unleash the extraordinary with the Samsung Gal...</td>\n",
       "      <td>The description flows well, using strong verbs...</td>\n",
       "      <td>good</td>\n",
       "      <td>The grammar in this description is excellent. ...</td>\n",
       "      <td>good</td>\n",
       "      <td>The tone is sophisticated and enthusiastic, ef...</td>\n",
       "      <td>good</td>\n",
       "      <td>This description is a good length. It highligh...</td>\n",
       "      <td>good</td>\n",
       "      <td>The description provides specific details abou...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Pixel 8 Pro</td>\n",
       "      <td>features: Tensor G3 chip, Magic Eraser, 50â€¯MP ...</td>\n",
       "      <td>matte glass back, aluminum frame</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "      <td>Meet the Google Pixel 8 Pro â€” the smartphone e...</td>\n",
       "      <td>The description flows well and is easy to unde...</td>\n",
       "      <td>good</td>\n",
       "      <td>The grammar in this description is excellent. ...</td>\n",
       "      <td>good</td>\n",
       "      <td>The tone is positive and enthusiastic, highlig...</td>\n",
       "      <td>good</td>\n",
       "      <td>This description is concise and effectively hi...</td>\n",
       "      <td>good</td>\n",
       "      <td>The description provides specific details like...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sony WHâ€‘1000XM5 Headphones</td>\n",
       "      <td>features: active noise cancelling, 30â€¯hr batte...</td>\n",
       "      <td>synthetic leather earcups</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "      <td>Immerse yourself in pure sound with the Sony W...</td>\n",
       "      <td>The description flows well and is easy to unde...</td>\n",
       "      <td>good</td>\n",
       "      <td>The grammar in this description is excellent. ...</td>\n",
       "      <td>good</td>\n",
       "      <td>The tone is slightly elevated and sophisticate...</td>\n",
       "      <td>good</td>\n",
       "      <td>This description is a good length. It provides...</td>\n",
       "      <td>good</td>\n",
       "      <td>The description provides specific details that...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bose QuietComfort Ultra Earbuds</td>\n",
       "      <td>features: CustomTune sound calibration, ANC, I...</td>\n",
       "      <td>silicone ear tips</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "      <td>Experience audio like never before with the Bo...</td>\n",
       "      <td>The description flows well and is easy to unde...</td>\n",
       "      <td>good</td>\n",
       "      <td>The grammar in this description is excellent. ...</td>\n",
       "      <td>good</td>\n",
       "      <td>The tone is positive and enthusiastic, highlig...</td>\n",
       "      <td>good</td>\n",
       "      <td>This description is a good length. It provides...</td>\n",
       "      <td>good</td>\n",
       "      <td>The description provides specific features tha...</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      product_name  \\\n",
       "0              Apple iPhone 15 Pro   \n",
       "1         Samsung Galaxy S24 Ultra   \n",
       "2               Google Pixel 8 Pro   \n",
       "3       Sony WHâ€‘1000XM5 Headphones   \n",
       "4  Bose QuietComfort Ultra Earbuds   \n",
       "\n",
       "                              Product_attribute_list  \\\n",
       "0  features: A17 Pro chip, 120â€¯Hz ProMotion displ...   \n",
       "1  features: 200â€¯MP camera, Sâ€‘Pen support, 120â€¯Hz...   \n",
       "2  features: Tensor G3 chip, Magic Eraser, 50â€¯MP ...   \n",
       "3  features: active noise cancelling, 30â€¯hr batte...   \n",
       "4  features: CustomTune sound calibration, ANC, I...   \n",
       "\n",
       "                                     material                 warranty  \\\n",
       "0        titanium frame, Ceramic Shield glass  1â€‘year limited warranty   \n",
       "1  Armor Aluminum frame, Gorilla Glass Victus  1â€‘year limited warranty   \n",
       "2            matte glass back, aluminum frame  1â€‘year limited warranty   \n",
       "3                   synthetic leather earcups  1â€‘year limited warranty   \n",
       "4                           silicone ear tips  1â€‘year limited warranty   \n",
       "\n",
       "                               generated_description  \\\n",
       "0  Unleash next-level performance with the Apple ...   \n",
       "1  Unleash the extraordinary with the Samsung Gal...   \n",
       "2  Meet the Google Pixel 8 Pro â€” the smartphone e...   \n",
       "3  Immerse yourself in pure sound with the Sony W...   \n",
       "4  Experience audio like never before with the Bo...   \n",
       "\n",
       "                                 Fluency_explanation Fluency_verdict  \\\n",
       "0  The description flows well and is easy to unde...            good   \n",
       "1  The description flows well, using strong verbs...            good   \n",
       "2  The description flows well and is easy to unde...            good   \n",
       "3  The description flows well and is easy to unde...            good   \n",
       "4  The description flows well and is easy to unde...            good   \n",
       "\n",
       "                                 Grammar_explanation Grammar_verdict  \\\n",
       "0  The grammar in this description is excellent. ...            good   \n",
       "1  The grammar in this description is excellent. ...            good   \n",
       "2  The grammar in this description is excellent. ...            good   \n",
       "3  The grammar in this description is excellent. ...            good   \n",
       "4  The grammar in this description is excellent. ...            good   \n",
       "\n",
       "                                    Tone_explanation Tone_verdict  \\\n",
       "0  The tone is professional and enthusiastic, hig...         good   \n",
       "1  The tone is sophisticated and enthusiastic, ef...         good   \n",
       "2  The tone is positive and enthusiastic, highlig...         good   \n",
       "3  The tone is slightly elevated and sophisticate...         good   \n",
       "4  The tone is positive and enthusiastic, highlig...         good   \n",
       "\n",
       "                                  Length_explanation Length_verdict  \\\n",
       "0  This description is a good length. It highligh...           good   \n",
       "1  This description is a good length. It highligh...           good   \n",
       "2  This description is concise and effectively hi...           good   \n",
       "3  This description is a good length. It provides...           good   \n",
       "4  This description is a good length. It provides...           good   \n",
       "\n",
       "                               Grounding_explanation Grounding_verdict  \n",
       "0  The description provides specific details that...              good  \n",
       "1  The description provides specific details abou...              good  \n",
       "2  The description provides specific details like...              good  \n",
       "3  The description provides specific details that...              good  \n",
       "4  The description provides specific features tha...                ok  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------- STUDENT TODO -------------------------\n",
    "# Iterate over a DataFrame of product descriptions, run alt_jlm on each row,\n",
    "# and append explanation & verdict columns per criterion.\n",
    "#\n",
    "# Expected input format:\n",
    "#   df[\"description\"]  -> the text to judge\n",
    "#\n",
    "# Example output columns for criterion 'clarity':\n",
    "#   clarity_explanation , clarity_verdict\n",
    "#\n",
    "# Adjust variable names / saving as needed.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def judge_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    judged_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        desc = row[\"generated_description\"]\n",
    "        result = per_criterion_jlm(desc, \"gemini-1.5-pro\", criteria)  # {crit: {\"explanation\": ..., \"verdict\": ...}}\n",
    "        flat = row.to_dict()\n",
    "        for crit, vals in result.items():\n",
    "            flat[f\"{crit}_explanation\"] = vals[\"explanation\"]\n",
    "            flat[f\"{crit}_verdict\"] = vals[\"verdict\"]\n",
    "        judged_rows.append(flat)\n",
    "    return pd.DataFrame(judged_rows)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "df_scored = judge_dataframe(outputs_df)\n",
    "df_scored.to_csv(\"descriptions_with_judgments.csv\", index=False)\n",
    "df_scored.head()\n",
    "# ---------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f6dad",
   "metadata": {
    "id": "c47f6dad"
   },
   "source": [
    "\n",
    "\n",
    "You now have a DataFrame with perâ€‘criterion **explanations** and **verdicts**.\n",
    "Next, add a final scoring pass that condenses those signals into one overall\n",
    "label (`goodâ€¯|â€¯okâ€¯|â€¯bad`) per description.\n",
    "\n",
    "### Judgement guidelines object\n",
    "\n",
    "Provide a dict with **exactly these criteria** and a Boolean indicating whether\n",
    "the criterion is a **noâ€‘go** (i.e. a *bad* verdict forces the overall label to\n",
    "*bad*):\n",
    "\n",
    "| Criterion   | Noâ€‘go? |\n",
    "|-------------|--------|\n",
    "| Fluency     | False  |\n",
    "| Grammar     | True?  |\n",
    "| Tone        | False  |\n",
    "| Length      | False  |\n",
    "| Grounding   | True?  |\n",
    "\n",
    "Adjust the True/False flags to suit your project.\n",
    "\n",
    "```python\n",
    "judgement_guidelines = {\n",
    "    \"Fluency\":   False,\n",
    "    \"Grammar\":   True,\n",
    "    \"Tone\":      False,\n",
    "    \"Length\":    False,\n",
    "    \"Grounding\": True,\n",
    "}\n",
    "```\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Implement `apply_final_scoring(df, judgement_guidelines)` (cell below).  \n",
    "2. Ensure it adds a `final_label` column using noâ€‘go rules plus an aggregated\n",
    "   score from the remaining criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "465eae5f",
   "metadata": {
    "id": "465eae5f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>Product_attribute_list</th>\n",
       "      <th>material</th>\n",
       "      <th>warranty</th>\n",
       "      <th>generated_description</th>\n",
       "      <th>Fluency_explanation</th>\n",
       "      <th>Fluency_verdict</th>\n",
       "      <th>Grammar_explanation</th>\n",
       "      <th>Grammar_verdict</th>\n",
       "      <th>Tone_explanation</th>\n",
       "      <th>Tone_verdict</th>\n",
       "      <th>Length_explanation</th>\n",
       "      <th>Length_verdict</th>\n",
       "      <th>Grounding_explanation</th>\n",
       "      <th>Grounding_verdict</th>\n",
       "      <th>final_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple iPhone 15 Pro</td>\n",
       "      <td>features: A17 Pro chip, 120â€¯Hz ProMotion displ...</td>\n",
       "      <td>titanium frame, Ceramic Shield glass</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "      <td>Unleash next-level performance with the Apple ...</td>\n",
       "      <td>The description flows well and is easy to unde...</td>\n",
       "      <td>good</td>\n",
       "      <td>The grammar in this description is excellent. ...</td>\n",
       "      <td>good</td>\n",
       "      <td>The tone is professional and enthusiastic, hig...</td>\n",
       "      <td>good</td>\n",
       "      <td>This description is a good length. It highligh...</td>\n",
       "      <td>good</td>\n",
       "      <td>The description provides specific details that...</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samsung Galaxy S24 Ultra</td>\n",
       "      <td>features: 200â€¯MP camera, Sâ€‘Pen support, 120â€¯Hz...</td>\n",
       "      <td>Armor Aluminum frame, Gorilla Glass Victus</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "      <td>Unleash the extraordinary with the Samsung Gal...</td>\n",
       "      <td>The description flows well, using strong verbs...</td>\n",
       "      <td>good</td>\n",
       "      <td>The grammar in this description is excellent. ...</td>\n",
       "      <td>good</td>\n",
       "      <td>The tone is sophisticated and enthusiastic, ef...</td>\n",
       "      <td>good</td>\n",
       "      <td>This description is a good length. It highligh...</td>\n",
       "      <td>good</td>\n",
       "      <td>The description provides specific details abou...</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Pixel 8 Pro</td>\n",
       "      <td>features: Tensor G3 chip, Magic Eraser, 50â€¯MP ...</td>\n",
       "      <td>matte glass back, aluminum frame</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "      <td>Meet the Google Pixel 8 Pro â€” the smartphone e...</td>\n",
       "      <td>The description flows well and is easy to unde...</td>\n",
       "      <td>good</td>\n",
       "      <td>The grammar in this description is excellent. ...</td>\n",
       "      <td>good</td>\n",
       "      <td>The tone is positive and enthusiastic, highlig...</td>\n",
       "      <td>good</td>\n",
       "      <td>This description is concise and effectively hi...</td>\n",
       "      <td>good</td>\n",
       "      <td>The description provides specific details like...</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sony WHâ€‘1000XM5 Headphones</td>\n",
       "      <td>features: active noise cancelling, 30â€¯hr batte...</td>\n",
       "      <td>synthetic leather earcups</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "      <td>Immerse yourself in pure sound with the Sony W...</td>\n",
       "      <td>The description flows well and is easy to unde...</td>\n",
       "      <td>good</td>\n",
       "      <td>The grammar in this description is excellent. ...</td>\n",
       "      <td>good</td>\n",
       "      <td>The tone is slightly elevated and sophisticate...</td>\n",
       "      <td>good</td>\n",
       "      <td>This description is a good length. It provides...</td>\n",
       "      <td>good</td>\n",
       "      <td>The description provides specific details that...</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bose QuietComfort Ultra Earbuds</td>\n",
       "      <td>features: CustomTune sound calibration, ANC, I...</td>\n",
       "      <td>silicone ear tips</td>\n",
       "      <td>1â€‘year limited warranty</td>\n",
       "      <td>Experience audio like never before with the Bo...</td>\n",
       "      <td>The description flows well and is easy to unde...</td>\n",
       "      <td>good</td>\n",
       "      <td>The grammar in this description is excellent. ...</td>\n",
       "      <td>good</td>\n",
       "      <td>The tone is positive and enthusiastic, highlig...</td>\n",
       "      <td>good</td>\n",
       "      <td>This description is a good length. It provides...</td>\n",
       "      <td>good</td>\n",
       "      <td>The description provides specific features tha...</td>\n",
       "      <td>ok</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      product_name  \\\n",
       "0              Apple iPhone 15 Pro   \n",
       "1         Samsung Galaxy S24 Ultra   \n",
       "2               Google Pixel 8 Pro   \n",
       "3       Sony WHâ€‘1000XM5 Headphones   \n",
       "4  Bose QuietComfort Ultra Earbuds   \n",
       "\n",
       "                              Product_attribute_list  \\\n",
       "0  features: A17 Pro chip, 120â€¯Hz ProMotion displ...   \n",
       "1  features: 200â€¯MP camera, Sâ€‘Pen support, 120â€¯Hz...   \n",
       "2  features: Tensor G3 chip, Magic Eraser, 50â€¯MP ...   \n",
       "3  features: active noise cancelling, 30â€¯hr batte...   \n",
       "4  features: CustomTune sound calibration, ANC, I...   \n",
       "\n",
       "                                     material                 warranty  \\\n",
       "0        titanium frame, Ceramic Shield glass  1â€‘year limited warranty   \n",
       "1  Armor Aluminum frame, Gorilla Glass Victus  1â€‘year limited warranty   \n",
       "2            matte glass back, aluminum frame  1â€‘year limited warranty   \n",
       "3                   synthetic leather earcups  1â€‘year limited warranty   \n",
       "4                           silicone ear tips  1â€‘year limited warranty   \n",
       "\n",
       "                               generated_description  \\\n",
       "0  Unleash next-level performance with the Apple ...   \n",
       "1  Unleash the extraordinary with the Samsung Gal...   \n",
       "2  Meet the Google Pixel 8 Pro â€” the smartphone e...   \n",
       "3  Immerse yourself in pure sound with the Sony W...   \n",
       "4  Experience audio like never before with the Bo...   \n",
       "\n",
       "                                 Fluency_explanation Fluency_verdict  \\\n",
       "0  The description flows well and is easy to unde...            good   \n",
       "1  The description flows well, using strong verbs...            good   \n",
       "2  The description flows well and is easy to unde...            good   \n",
       "3  The description flows well and is easy to unde...            good   \n",
       "4  The description flows well and is easy to unde...            good   \n",
       "\n",
       "                                 Grammar_explanation Grammar_verdict  \\\n",
       "0  The grammar in this description is excellent. ...            good   \n",
       "1  The grammar in this description is excellent. ...            good   \n",
       "2  The grammar in this description is excellent. ...            good   \n",
       "3  The grammar in this description is excellent. ...            good   \n",
       "4  The grammar in this description is excellent. ...            good   \n",
       "\n",
       "                                    Tone_explanation Tone_verdict  \\\n",
       "0  The tone is professional and enthusiastic, hig...         good   \n",
       "1  The tone is sophisticated and enthusiastic, ef...         good   \n",
       "2  The tone is positive and enthusiastic, highlig...         good   \n",
       "3  The tone is slightly elevated and sophisticate...         good   \n",
       "4  The tone is positive and enthusiastic, highlig...         good   \n",
       "\n",
       "                                  Length_explanation Length_verdict  \\\n",
       "0  This description is a good length. It highligh...           good   \n",
       "1  This description is a good length. It highligh...           good   \n",
       "2  This description is concise and effectively hi...           good   \n",
       "3  This description is a good length. It provides...           good   \n",
       "4  This description is a good length. It provides...           good   \n",
       "\n",
       "                               Grounding_explanation Grounding_verdict  \\\n",
       "0  The description provides specific details that...              good   \n",
       "1  The description provides specific details abou...              good   \n",
       "2  The description provides specific details like...              good   \n",
       "3  The description provides specific details that...              good   \n",
       "4  The description provides specific features tha...                ok   \n",
       "\n",
       "  final_label  \n",
       "0        good  \n",
       "1        good  \n",
       "2        good  \n",
       "3        good  \n",
       "4        good  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------- STUDENT TODO -------------------------\n",
    "# Postâ€‘processing: compute a rowâ€‘level final score using guideline rules.\n",
    "# Inputs:\n",
    "#   df                â€“ the DataFrame returned by judge_dataframe()\n",
    "#   judgement_guidelines â€“ dict mapping criterion -> bool\n",
    "#                          True  => \"noâ€‘go\" criterion (a 'bad' verdict => overall bad)\n",
    "#                          False => normal criterion\n",
    "#\n",
    "# Strategy (example â€“ feel free to tweak):\n",
    "#   â€¢ Map verdicts to numeric scores {good: 2, ok: 1, bad: 0}.\n",
    "#   â€¢ If any noâ€‘go criterion is 'bad' => overall_label = 'bad' (score 0).\n",
    "#   â€¢ Otherwise sum numeric scores, normalise by (2 * #criteria) to get 0â€‘1 range,\n",
    "#       and convert back to overall_label ('good' â‰¥ 0.75, 'ok' â‰¥ 0.4 else 'bad').\n",
    "\n",
    "VERDICT_TO_SCORE = {\"good\": 2, \"ok\": 1, \"bad\": 0}\n",
    "\n",
    "def apply_final_scoring(df, judgement_guidelines: dict) -> pd.DataFrame:\n",
    "    scored_rows = []\n",
    "    crits = [c for c in judgement_guidelines.keys()]\n",
    "    for _, row in df.iterrows():\n",
    "        # Check noâ€‘go first\n",
    "        overall_label = None\n",
    "        for crit in crits:\n",
    "            v = row.get(f\"{crit}_verdict\", \"\").lower()\n",
    "            if judgement_guidelines[crit] and v == \"bad\":\n",
    "                overall_label = \"bad\"\n",
    "                break\n",
    "        # If not nailed by noâ€‘go, compute aggregate\n",
    "        if overall_label is None:\n",
    "            total = 0\n",
    "            max_total = 2 * len(crits)\n",
    "            for crit in crits:\n",
    "                total += VERDICT_TO_SCORE.get(row.get(f\"{crit}_verdict\", \"\").lower(), 0)\n",
    "            norm = total / max_total\n",
    "            if norm >= 0.75:\n",
    "                overall_label = \"good\"\n",
    "            elif norm >= 0.4:\n",
    "                overall_label = \"ok\"\n",
    "            else:\n",
    "                overall_label = \"bad\"\n",
    "        scored_row = row.to_dict()\n",
    "        scored_row[\"final_label\"] = overall_label\n",
    "        scored_rows.append(scored_row)\n",
    "    return pd.DataFrame(scored_rows)\n",
    "\n",
    "# Example usage:\n",
    "# guidelines = {\"clarity\": False, \"factuality\": True, \"persuasiveness\": False}\n",
    "# df_scored = apply_final_scoring(df_scored, guidelines)\n",
    "# df_scored.head()\n",
    "# ---------------------------------------------------------------\n",
    "judgement_guidelines = {\n",
    "    \"Fluency\": False,\n",
    "    \"Grammar\": True,\n",
    "    \"Tone\": False,\n",
    "    \"Length\": False,\n",
    "    \"Grounding\": True,\n",
    "}\n",
    "\n",
    "final_score_df = apply_final_scoring(df_scored, judgement_guidelines)\n",
    "final_score_df.to_csv(\"final_with_judgments.csv\", index=False)\n",
    "final_score_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e9ccf",
   "metadata": {
    "id": "419e9ccf"
   },
   "source": [
    "## 6Â Majorityâ€‘VoteÂ Ensemble - NOT EASY\n",
    "\n",
    "Ensembling is a simple way to boost robustness.  \n",
    "Here we will take **multiple independent calls** to the **alternate judge model** you built in SectionÂ 4 (`per_criterion_jlm`) and\n",
    "combine their verdicts with a majority vote.\n",
    "\n",
    "*Call *different* `models` several times so that each run can produce (hopefully) different explanations/verdicts.*\n",
    "\n",
    "### Your task\n",
    "\n",
    "1. Implement **`majority_vote_jlm(description, n=3)`** that:\n",
    "   * Invokes `per_criterion_jlm(description)` with **n** different models (minimum of 3),\n",
    "   * Return a dictionary with all results that will later be written to a pd DF.\n",
    "2. Test it on the same product description you used in SectionsÂ 3â€“4.\n",
    "3. Briefly comment: *Does majority voting smooth out noisy judgments?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "08436d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Section 6 â€“ Majority-Vote Ensemble (alternate provider)\n",
    "# =========================\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# If you already defined _MODEL_POOL earlier, this will keep yours.\n",
    "if \"_MODEL_POOL\" not in globals():\n",
    "    _MODEL_POOL = {\n",
    "        # Names here should match what your call_model() accepts for the given provider\n",
    "        \"gemini\": [\n",
    "            \"models/gemini-1.5-pro\",\n",
    "            \"models/gemini-1.5-flash\",\n",
    "        ],\n",
    "        \"openai\": [\n",
    "            \"chatgpt-4o-latest\",\n",
    "            \"gpt-4o\",\n",
    "        ]\n",
    "    }\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def majority_vote_jlm(\n",
    "    description: str,\n",
    "    n: int = 3,\n",
    "    alt_backend: str | None = None,        # kept for compatibility; not required when mixing\n",
    "    criteria_list: list[str] | None = None,\n",
    "    seed: int = 42,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Mixed-provider majority vote: samples models from OpenAI + Gemini,\n",
    "    calls the right provider per model, aggregates label majorities,\n",
    "    and picks model winners by count of 'good' verdicts.\n",
    "    \"\"\"\n",
    "    # --- model pools ---\n",
    "    pool_openai = (globals().get(\"_MODEL_POOL\", {}).get(\"openai\") or [\n",
    "        # Use API model IDs, not ChatGPT UI aliases:\n",
    "        \"gpt-4o\", \"gpt-5\"  # adjust to what you have access to\n",
    "    ])\n",
    "    pool_gemini = (globals().get(\"_MODEL_POOL\", {}).get(\"gemini\") or [\n",
    "        \"models/gemini-1.5-pro\", \"models/gemini-1.5-flash\"\n",
    "    ])\n",
    "    model_pool = list(pool_openai) + list(pool_gemini)\n",
    "    if not model_pool:\n",
    "        raise ValueError(\"No models configured in _MODEL_POOL for openai/gemini.\")\n",
    "\n",
    "    # Map each model to its provider\n",
    "    provider_by_model = {m: \"openai\" for m in pool_openai}\n",
    "    provider_by_model.update({m: \"gemini\" for m in pool_gemini})\n",
    "\n",
    "    # Sample n models (distinct if possible)\n",
    "    random.seed(seed)\n",
    "    models = random.sample(model_pool, n) if n <= len(model_pool) else random.choices(model_pool, k=n)\n",
    "\n",
    "    # Resolve criteria\n",
    "    if criteria_list is None:\n",
    "        criteria_list = globals().get(\"criteria\", [\"Fluency\", \"Grammar\", \"Tone\", \"Length\", \"Grounding\"])\n",
    "\n",
    "    per_model_results: dict[str, dict] = {}\n",
    "\n",
    "    # Save current BACKEND; we will switch per model\n",
    "    orig_backend = globals().get(\"BACKEND\", None)\n",
    "\n",
    "    for m in models:\n",
    "        provider = provider_by_model.get(m)\n",
    "        if provider is None:\n",
    "            # fallback: leave BACKEND unchanged but warn inside result\n",
    "            provider = orig_backend or \"openai\"\n",
    "        try:\n",
    "            globals()[\"BACKEND\"] = provider  # make call_model() route to the right API\n",
    "            res = per_criterion_jlm(description, m, criteria_list)\n",
    "        except Exception as e:\n",
    "            res = {c: {\"explanation\": f\"ERROR: {e}\", \"verdict\": None} for c in criteria_list}\n",
    "        per_model_results[m] = res\n",
    "\n",
    "    # Restore BACKEND\n",
    "    if orig_backend is not None:\n",
    "        globals()[\"BACKEND\"] = orig_backend\n",
    "\n",
    "    # --- majority aggregation per criterion (label-based) ---\n",
    "    def _vote_winner(labels: list[str]) -> tuple[str | None, bool]:\n",
    "        clean = [str(x).strip().lower() for x in labels if x]\n",
    "        if not clean:\n",
    "            return None, False\n",
    "        counts = Counter(clean)\n",
    "        top = counts.most_common()\n",
    "        if len(top) == 1 or top[0][1] > top[1][1]:\n",
    "            return top[0][0], False\n",
    "        tied = {lbl for lbl, c in top if c == top[0][1]}\n",
    "        return (\"ok\", True) if \"ok\" in tied else (top[0][0], True)\n",
    "\n",
    "    majority: dict[str, dict] = {}\n",
    "    for crit in criteria_list:\n",
    "        labels = []\n",
    "        by_label_expls = {\"good\": [], \"ok\": [], \"bad\": []}\n",
    "        for m in models:\n",
    "            entry = (per_model_results.get(m, {}) or {}).get(crit, {}) or {}\n",
    "            v = entry.get(\"verdict\")\n",
    "            if v:\n",
    "                vv = str(v).strip().lower()\n",
    "                labels.append(vv)\n",
    "                if vv in by_label_expls:\n",
    "                    by_label_expls[vv].append(entry.get(\"explanation\"))\n",
    "        winner_label, tie = _vote_winner(labels)\n",
    "        counts = Counter([x for x in labels if x])\n",
    "        w_expl = (by_label_expls.get(winner_label) or [None])[0] if winner_label in by_label_expls else None\n",
    "        majority[crit] = {\n",
    "            \"winner\": winner_label,\n",
    "            \"tie\": bool(tie),\n",
    "            \"counts\": {\"good\": counts.get(\"good\", 0), \"ok\": counts.get(\"ok\", 0), \"bad\": counts.get(\"bad\", 0)},\n",
    "            \"winner_explanation\": w_expl,\n",
    "        }\n",
    "\n",
    "    # --- model-based winners (your new rule) ---\n",
    "    def _count_label(model_name: str, label: str) -> int:\n",
    "        return sum(\n",
    "            1 for crit in criteria_list\n",
    "            if str(((per_model_results.get(model_name, {}) or {}).get(crit, {}) or {}).get(\"verdict\") or \"\")\n",
    "               .strip().lower() == label\n",
    "        )\n",
    "\n",
    "    good_counts_by_model = {m: _count_label(m, \"good\") for m in models}\n",
    "    ok_counts_by_model   = {m: _count_label(m, \"ok\")   for m in models}  # tie-break only\n",
    "\n",
    "    if models:\n",
    "        max_good = max(good_counts_by_model.values())\n",
    "        contenders = [m for m, c in good_counts_by_model.items() if c == max_good]\n",
    "        if len(contenders) > 1:\n",
    "            best_ok = max(ok_counts_by_model[m] for m in contenders)\n",
    "            contenders = [m for m in contenders if ok_counts_by_model[m] == best_ok]\n",
    "        overall_winner_model = sorted(contenders)[0] if contenders else None\n",
    "    else:\n",
    "        overall_winner_model = None\n",
    "\n",
    "    criterion_winner_model: dict[str, str | None] = {}\n",
    "    for crit in criteria_list:\n",
    "        good_models = [\n",
    "            m for m in models\n",
    "            if str(((per_model_results.get(m, {}) or {}).get(crit, {}) or {}).get(\"verdict\") or \"\")\n",
    "               .strip().lower() == \"good\"\n",
    "        ]\n",
    "        criterion_winner_model[crit] = good_models[0] if good_models else None\n",
    "\n",
    "    return {\n",
    "        \"backend\": \"mixed(openai+gemini)\",\n",
    "        \"models\": models,\n",
    "        \"per_model\": per_model_results,\n",
    "        \"majority\": majority,\n",
    "        \"overall_winner_model\": overall_winner_model,\n",
    "        \"good_counts_by_model\": good_counts_by_model,\n",
    "        \"ok_counts_by_model\": ok_counts_by_model,\n",
    "        \"criterion_winner_model\": criterion_winner_model,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9039be00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models used: ['models/gemini-1.5-flash-8b', 'chatgpt-4o-latest', 'models/gemini-1.5-flash']\n",
      "Overall winner (most 'good'): chatgpt-4o-latest | good counts: {'models/gemini-1.5-flash-8b': 0, 'chatgpt-4o-latest': 5, 'models/gemini-1.5-flash': 3}\n",
      "- Fluency   : winner_model=chatgpt-4o-latest, counts={'good': 2, 'ok': 1, 'bad': 0} (label_winner=good)\n",
      "- Grammar   : winner_model=chatgpt-4o-latest, counts={'good': 2, 'ok': 1, 'bad': 0} (label_winner=good)\n",
      "- Tone      : winner_model=chatgpt-4o-latest, counts={'good': 1, 'ok': 2, 'bad': 0} (label_winner=ok)\n",
      "- Length    : winner_model=chatgpt-4o-latest, counts={'good': 2, 'ok': 1, 'bad': 0} (label_winner=good)\n",
      "- Grounding : winner_model=chatgpt-4o-latest, counts={'good': 1, 'ok': 0, 'bad': 2} (label_winner=bad)\n"
     ]
    }
   ],
   "source": [
    "# Re-use the same product description as Sections 3â€“4.\n",
    "# If you already have `description` defined above, this will use it.\n",
    "try:\n",
    "    description\n",
    "except NameError:\n",
    "    description = \"A compact, wireless Bluetooth speaker with 12-hour battery, IPX7 water resistance, and USB-C fast charging.\"\n",
    "\n",
    "mv = majority_vote_jlm(description, n=3, alt_backend=\"gemini\")\n",
    "print(\"Models used:\", mv[\"models\"])\n",
    "print(\"Overall winner (most 'good'):\", mv[\"overall_winner_model\"], \"| good counts:\", mv[\"good_counts_by_model\"])\n",
    "for c in mv[\"majority\"]:\n",
    "    print(f\"- {c:10s}: winner_model={mv['criterion_winner_model'][c]}, counts={mv['majority'][c]['counts']} (label_winner={mv['majority'][c]['winner']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8c2db587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helpers: summarize a majority-vote result to a tidy DataFrame ---\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def _mv_to_df(mv: dict) -> pd.DataFrame:\n",
    "    \"\"\"Convert majority_vote_jlm() output to a tidy DF.\"\"\"\n",
    "    rows = []\n",
    "    for crit, agg in mv[\"majority\"].items():\n",
    "        counts = agg.get(\"counts\", {}) or {}\n",
    "        rows.append({\n",
    "            \"backend\": mv.get(\"backend\"),\n",
    "            \"models_used\": \", \".join(mv.get(\"models\", [])),\n",
    "            \"criterion\": crit,\n",
    "            \"good\": int(counts.get(\"good\", 0)),\n",
    "            \"ok\": int(counts.get(\"ok\", 0)),\n",
    "            \"bad\": int(counts.get(\"bad\", 0)),\n",
    "            \"winner\": agg.get(\"winner\"),\n",
    "            \"tie\": bool(agg.get(\"tie\")),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def compare_backends(description: str, n: int = 3, backends=(\"openai\",\"gemini\"), seed: int = 42):\n",
    "    \"\"\"\n",
    "    Run majority_vote_jlm on multiple providers and return:\n",
    "      - results: dict of backend -> raw mv dict\n",
    "      - df: tidy DataFrame with counts/winner per criterion and backend\n",
    "      - wide: pivoted table (criterion as rows; backend columns for Good/OK/Bad)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    frames = []\n",
    "    for b in backends:\n",
    "        mv = majority_vote_jlm(description, n=n, alt_backend=b, seed=seed)\n",
    "        results[b] = mv\n",
    "        frames.append(_mv_to_df(mv))\n",
    "\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    # Nice wide comparison for Good/OK/Bad\n",
    "    wide = (df\n",
    "            .set_index([\"criterion\",\"backend\"])\n",
    "            [[\"good\",\"ok\",\"bad\",\"winner\",\"tie\"]]\n",
    "            .reset_index())\n",
    "    return results, df, wide\n",
    "\n",
    "def print_backend_summaries(results: dict):\n",
    "    \"\"\"Print the same summary lines you used before, grouped by backend.\"\"\"\n",
    "    for backend, mv in results.items():\n",
    "        print(f\"\\n=== {backend.upper()} ===\")\n",
    "        print(\"Models used:\", mv[\"models\"])\n",
    "        for crit, agg in mv[\"majority\"].items():\n",
    "            print(f\"- {crit:10s}: winner={agg['winner']}, counts={agg['counts']}, tie={agg['tie']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "265f13eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== OPENAI ===\n",
      "Models used: ['gpt-4o-mini', 'chatgpt-4o-latest', 'gpt-4o']\n",
      "- Fluency   : winner=good, counts={'good': 3, 'ok': 0, 'bad': 0}, tie=False\n",
      "- Grammar   : winner=good, counts={'good': 2, 'ok': 1, 'bad': 0}, tie=False\n",
      "- Tone      : winner=good, counts={'good': 2, 'ok': 1, 'bad': 0}, tie=False\n",
      "- Length    : winner=good, counts={'good': 2, 'ok': 1, 'bad': 0}, tie=False\n",
      "- Grounding : winner=good, counts={'good': 2, 'ok': 1, 'bad': 0}, tie=False\n",
      "\n",
      "=== GEMINI ===\n",
      "Models used: ['models/gemini-1.5-flash-8b', 'models/gemini-1.5-pro', 'models/gemini-1.5-flash']\n",
      "- Fluency   : winner=good, counts={'good': 2, 'ok': 1, 'bad': 0}, tie=False\n",
      "- Grammar   : winner=good, counts={'good': 2, 'ok': 1, 'bad': 0}, tie=False\n",
      "- Tone      : winner=ok, counts={'good': 0, 'ok': 3, 'bad': 0}, tie=False\n",
      "- Length    : winner=good, counts={'good': 2, 'ok': 1, 'bad': 0}, tie=False\n",
      "- Grounding : winner=bad, counts={'good': 1, 'ok': 0, 'bad': 2}, tie=False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backend</th>\n",
       "      <th>models_used</th>\n",
       "      <th>criterion</th>\n",
       "      <th>good</th>\n",
       "      <th>ok</th>\n",
       "      <th>bad</th>\n",
       "      <th>winner</th>\n",
       "      <th>tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gemini</td>\n",
       "      <td>models/gemini-1.5-flash-8b, models/gemini-1.5-...</td>\n",
       "      <td>Fluency</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>good</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4o-mini, chatgpt-4o-latest, gpt-4o</td>\n",
       "      <td>Fluency</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>good</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gemini</td>\n",
       "      <td>models/gemini-1.5-flash-8b, models/gemini-1.5-...</td>\n",
       "      <td>Grammar</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>good</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4o-mini, chatgpt-4o-latest, gpt-4o</td>\n",
       "      <td>Grammar</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>good</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gemini</td>\n",
       "      <td>models/gemini-1.5-flash-8b, models/gemini-1.5-...</td>\n",
       "      <td>Grounding</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>bad</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4o-mini, chatgpt-4o-latest, gpt-4o</td>\n",
       "      <td>Grounding</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>good</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gemini</td>\n",
       "      <td>models/gemini-1.5-flash-8b, models/gemini-1.5-...</td>\n",
       "      <td>Length</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>good</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4o-mini, chatgpt-4o-latest, gpt-4o</td>\n",
       "      <td>Length</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>good</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gemini</td>\n",
       "      <td>models/gemini-1.5-flash-8b, models/gemini-1.5-...</td>\n",
       "      <td>Tone</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>ok</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4o-mini, chatgpt-4o-latest, gpt-4o</td>\n",
       "      <td>Tone</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>good</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  backend                                        models_used  criterion  good  \\\n",
       "5  gemini  models/gemini-1.5-flash-8b, models/gemini-1.5-...    Fluency     2   \n",
       "0  openai             gpt-4o-mini, chatgpt-4o-latest, gpt-4o    Fluency     3   \n",
       "6  gemini  models/gemini-1.5-flash-8b, models/gemini-1.5-...    Grammar     2   \n",
       "1  openai             gpt-4o-mini, chatgpt-4o-latest, gpt-4o    Grammar     2   \n",
       "9  gemini  models/gemini-1.5-flash-8b, models/gemini-1.5-...  Grounding     1   \n",
       "4  openai             gpt-4o-mini, chatgpt-4o-latest, gpt-4o  Grounding     2   \n",
       "8  gemini  models/gemini-1.5-flash-8b, models/gemini-1.5-...     Length     2   \n",
       "3  openai             gpt-4o-mini, chatgpt-4o-latest, gpt-4o     Length     2   \n",
       "7  gemini  models/gemini-1.5-flash-8b, models/gemini-1.5-...       Tone     0   \n",
       "2  openai             gpt-4o-mini, chatgpt-4o-latest, gpt-4o       Tone     2   \n",
       "\n",
       "   ok  bad winner    tie  \n",
       "5   1    0   good  False  \n",
       "0   0    0   good  False  \n",
       "6   1    0   good  False  \n",
       "1   1    0   good  False  \n",
       "9   0    2    bad  False  \n",
       "4   1    0   good  False  \n",
       "8   1    0   good  False  \n",
       "3   1    0   good  False  \n",
       "7   3    0     ok  False  \n",
       "2   1    0   good  False  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reuse your description from Sections 3â€“4, or define one:\n",
    "try:\n",
    "    description\n",
    "except NameError:\n",
    "    description = \"A compact, wireless Bluetooth speaker with 12-hour battery, IPX7 water resistance, and USB-C fast charging.\"\n",
    "\n",
    "# Try both providers; bump n if you want more smoothing (e.g., n=5)\n",
    "results, df_long, df_wide = compare_backends(description, n=3, backends=(\"openai\",\"gemini\"), seed=42)\n",
    "\n",
    "# Print the familiar summary lines for each provider\n",
    "print_backend_summaries(results)\n",
    "\n",
    "# See the tidy table\n",
    "df_long.sort_values([\"criterion\",\"backend\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "32c0bbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aggregate Good/OK/Bad sums per backend\n",
    "agg = df_long.groupby(\"backend\")[[\"good\",\"ok\",\"bad\"]].sum().reset_index()\n",
    "\n",
    "plt.figure()\n",
    "x = range(len(agg))\n",
    "w = 0.25\n",
    "plt.bar([xi - w for xi in x], agg[\"good\"], width=w, label=\"Good\")\n",
    "plt.bar(x, agg[\"ok\"], width=w, label=\"OK\")\n",
    "plt.bar([xi + w for xi in x], agg[\"bad\"], width=w, label=\"Bad\")\n",
    "plt.xticks(list(x), agg[\"backend\"])\n",
    "plt.ylabel(\"Counts (sum over all criteria)\")\n",
    "plt.title(\"OpenAI vs Gemini â€“ Majority-Vote Totals\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9dc46f",
   "metadata": {
    "id": "2a9dc46f"
   },
   "source": [
    "## 7Â *(Bonus)*Â Datasetâ€‘Level Majorityâ€‘Vote Evaluation\n",
    "\n",
    "You have a rowâ€‘level ensemble grader (`majority_vote_jlm`) that produces a more\n",
    "stable verdict for **one** product description.  \n",
    "In this bonus section youâ€™ll **scale** that ensemble to an entire DataFrame so\n",
    "you can compute datasetâ€‘level metrics.\n",
    "\n",
    "### Goals\n",
    "\n",
    "1. **Automate majority voting across the dataset**: run `majority_vote_jlm`\n",
    "   *n* times per description and store the combined verdicts/explanations.  \n",
    "2. **Reuse your scoring pipeline**: pipe the enriched DataFrame through\n",
    "   `apply_final_scoring` to get a `final_label`.  \n",
    "3. **Compare**: how do the label distributions differ from the singleâ€‘judge\n",
    "   evaluation in SectionÂ 5?\n",
    "\n",
    "> *Extra credit*: add timing & cost tracking so you can quantify the tradeâ€‘off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc415eb3",
   "metadata": {
    "id": "cc415eb3"
   },
   "outputs": [],
   "source": [
    "# ------------------------- BONUS TODO -------------------------\n",
    "import pandas as pd\n",
    "\n",
    "def judge_dataframe_mv(df: pd.DataFrame, n: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"Run majorityâ€‘vote JLM `n` times per description and return enriched DF.\"\"\"\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        desc = row[\"description\"]\n",
    "        result = majority_vote_jlm(desc, n=n)\n",
    "        flat = row.to_dict()\n",
    "        for crit, vals in result.items():\n",
    "            flat[f\"{crit}_explanation\"] = vals[\"explanation\"]\n",
    "            flat[f\"{crit}_verdict\"] = vals[\"verdict\"]\n",
    "        rows.append(flat)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Example workflow:\n",
    "# df_mv = judge_dataframe_mv(df_input, n=5)\n",
    "# df_mv_scored = apply_final_scoring(df_mv, judgement_guidelines)\n",
    "# print(df_mv_scored['final_label'].value_counts())\n",
    "# ---------------------------------------------------------------\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
