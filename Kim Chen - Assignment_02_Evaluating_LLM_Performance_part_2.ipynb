{"cells":[{"cell_type":"markdown","id":"8da9f1c7","metadata":{"id":"8da9f1c7"},"source":["# Assignment¬†2 ‚Äì Evaluating LLM Output Quality"]},{"cell_type":"markdown","id":"a0ac7b94","metadata":{"id":"a0ac7b94"},"source":["# Introduction to Large Language Models and Prompt Engineering\n","\n","**Course:** GenAI development and LLM applications\n","\n","\n","**Instructors:** Ori Shapira, Yuval Belfer\n","\n","**Semester:** Summer\n","    \n","## Overview\n","\n","This assignment provides a **hands‚Äëon** experince with the world of LLM based systems evaluation: from understanding the business use-case and defining evaluation criterias in light of it. To performing human evaluation and dealing with the hardships of \"non-objectivity\", trough experimenting with **JLMs** (Judge Language Models).\n","\n","Along the way you will explore the differnces between the two evaluation methods, thier advanteges and dis-advanteges and try to figure out how and when to use each further down your GenAI road.\n","\n","## Learning Objectives\n","\n","- **Define evaluation criteria** understand the importance of defining how to measure your system performance in a non closely defined problem.\n","- **Compare** manual vs. automatic common methods.\n","- **Drive improvement** through proper evaluation, documentation and change cycles.\n","- **Design** usable automatic evaluation pipeline.\n","\n","## Prerequisites\n","- Basic Python knowledge\n","- Familiarity with Jupyter notebooks\n","- Internet connection for API calls"]},{"cell_type":"markdown","id":"23df8bfc","metadata":{"id":"23df8bfc"},"source":["# Part 1 - Human evaluation"]},{"cell_type":"markdown","id":"0637f95c","metadata":{"id":"0637f95c"},"source":["## 1‚ÄØ¬†Setup"]},{"cell_type":"code","execution_count":1,"id":"3d1cafeb","metadata":{"executionInfo":{"elapsed":147153,"status":"ok","timestamp":1754149296311,"user":{"displayName":"Kim Chen","userId":"03693798735466240238"},"user_tz":-180},"id":"3d1cafeb"},"outputs":[],"source":["!pip -q install --upgrade \"transformers[torch]\" datasets accelerate bitsandbytes --progress-bar off"]},{"cell_type":"markdown","id":"b09b1f31","metadata":{"id":"b09b1f31"},"source":["## 2‚ÄØ¬†Business use case ‚Äì Generate Product Descriptions\n","Many e‚Äëcommerce sites need engaging **product descriptions**. Given structured attributes (name, category, features, color, price), your model should craft a persuasive, 50‚Äë90‚Äëword description."]},{"cell_type":"code","execution_count":3,"id":"94d31015","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":382},"executionInfo":{"elapsed":2920,"status":"error","timestamp":1754149322674,"user":{"displayName":"Kim Chen","userId":"03693798735466240238"},"user_tz":-180},"id":"94d31015","outputId":"76060c37-b672-4511-df86-77f4a6e59ead"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded 50 products\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>product_name</th>\n","      <th>Product_attribute_list</th>\n","      <th>material</th>\n","      <th>warranty</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Apple iPhone 15 Pro</td>\n","      <td>features: A17 Pro chip, 120‚ÄØHz ProMotion displ...</td>\n","      <td>titanium frame, Ceramic Shield glass</td>\n","      <td>1‚Äëyear limited warranty</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Samsung Galaxy S24 Ultra</td>\n","      <td>features: 200‚ÄØMP camera, S‚ÄëPen support, 120‚ÄØHz...</td>\n","      <td>Armor Aluminum frame, Gorilla Glass Victus</td>\n","      <td>1‚Äëyear limited warranty</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Google Pixel 8 Pro</td>\n","      <td>features: Tensor G3 chip, Magic Eraser, 50‚ÄØMP ...</td>\n","      <td>matte glass back, aluminum frame</td>\n","      <td>1‚Äëyear limited warranty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sony WH‚Äë1000XM5 Headphones</td>\n","      <td>features: active noise cancelling, 30‚ÄØhr batte...</td>\n","      <td>synthetic leather earcups</td>\n","      <td>1‚Äëyear limited warranty</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Bose QuietComfort Ultra Earbuds</td>\n","      <td>features: CustomTune sound calibration, ANC, I...</td>\n","      <td>silicone ear tips</td>\n","      <td>1‚Äëyear limited warranty</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                      product_name  \\\n","0              Apple iPhone 15 Pro   \n","1         Samsung Galaxy S24 Ultra   \n","2               Google Pixel 8 Pro   \n","3       Sony WH‚Äë1000XM5 Headphones   \n","4  Bose QuietComfort Ultra Earbuds   \n","\n","                              Product_attribute_list  \\\n","0  features: A17 Pro chip, 120‚ÄØHz ProMotion displ...   \n","1  features: 200‚ÄØMP camera, S‚ÄëPen support, 120‚ÄØHz...   \n","2  features: Tensor G3 chip, Magic Eraser, 50‚ÄØMP ...   \n","3  features: active noise cancelling, 30‚ÄØhr batte...   \n","4  features: CustomTune sound calibration, ANC, I...   \n","\n","                                     material                 warranty  \n","0        titanium frame, Ceramic Shield glass  1‚Äëyear limited warranty  \n","1  Armor Aluminum frame, Gorilla Glass Victus  1‚Äëyear limited warranty  \n","2            matte glass back, aluminum frame  1‚Äëyear limited warranty  \n","3                   synthetic leather earcups  1‚Äëyear limited warranty  \n","4                           silicone ear tips  1‚Äëyear limited warranty  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Load the product dataset\n","import pandas as pd\n","\n","dataset_path = \"Assignment_02_product_dataset.csv\"  # ensure the file is uploaded\n","df_products = pd.read_csv(dataset_path)\n","print(f\"Loaded {len(df_products)} products\")\n","df_products.head()"]},{"cell_type":"markdown","id":"3f0fb841","metadata":{"id":"3f0fb841"},"source":["## 3‚ÄØ¬†Evaluation criteria\n","| Criterion | Description | Rating |\n","|-----------|-------------|--------|\n","| **Fluency** | Natural, easy‚Äëto‚Äëread sentences | good / ok / bad |\n","| **Grammar** | Correct spelling & punctuation | good / ok / bad |\n","| **Tone** | Matches friendly, credible sales voice | good / ok / bad |\n","| **Length** | 50‚Äë90 words | good / ok / bad |\n","| **Grounding** | Sticks to provided attributes only | good / ok / bad |\n","| **Latency** | Time to first byte / full response | good / ok / bad  (based on avg. time per call)|\n","| **Cost** | Relative inference or API cost per 1K tokens | good / ok / bad (based on avg. price per cal)|\n","\n","**Define your rubric:**\n","1. For each criterion, spell out what qualifies as **good**, **ok**, and **bad** to minimize subjectivity (e.g. for *Length*: good‚ÄØ=‚ÄØ50‚Äë90‚ÄØwords, ok‚ÄØ=‚ÄØ40‚Äë49‚ÄØor‚ÄØ91‚Äë110‚ÄØwords, bad‚ÄØ=‚ÄØoutside that range).\n","2. Decide the **cumulative pass bar**‚Äîfor instance, at least three *good* ratings and no *bad* ratings overall.\n","3. Establish **go / no‚Äëgo rules**‚Äîe.g. if *Grounding* is *bad* the description is automatically rejected, regardless of other scores."]},{"cell_type":"markdown","id":"ff8534af","metadata":{"id":"ff8534af"},"source":["## 4‚ÄØ¬†Prompt\n","\n","üí° **Prompt‚Äëengineering tip:**\n","Feel free to iterate on the prompt to maximize output quality. You can:\n","- Add a **system message** that defines writing style, brand voice, or formatting rules.\n","- Provide **one or two high‚Äëquality examples** (few‚Äëshot) of attribute‚Üídescription pairs.\n","- Include explicit constraints (word count, tone adjectives, banned phrases).\n","- Experiment with phrases like *\"Think step‚Äëby‚Äëstep\"* or *\"First reason, then answer\"*.\n","\n","Document any changes you make and observe how they influence the evaluation scores."]},{"cell_type":"code","execution_count":4,"id":"bc0da72b","metadata":{"id":"bc0da72b"},"outputs":[{"name":"stderr","output_type":"stream","text":["<>:4: SyntaxWarning: invalid escape sequence '\\M'\n","<>:4: SyntaxWarning: invalid escape sequence '\\M'\n","/var/folders/8k/xt0vhllx4jj6srjchtkrplr40000gn/T/ipykernel_48965/3038005655.py:4: SyntaxWarning: invalid escape sequence '\\M'\n","  \"Product name: {product_name}\\nFeatures: {Product_attribute_list}\\Material: {material}\\nWarranty: {warranty}\\n\\n\"\n"]}],"source":["prompt_tmpl = (\n","    \"You are a copywriter for an online store. Using the product attributes, \"\n","    \"write an engaging product description (50‚Äì90 words).\\n\\n\"\n","    \"Product name: {product_name}\\nFeatures: {Product_attribute_list}\\Material: {material}\\nWarranty: {warranty}\\n\\n\"\n","    \"Description:\"\n",")"]},{"cell_type":"markdown","id":"9b396031","metadata":{"id":"9b396031"},"source":["## 5‚ÄØ¬†Run a medium‚Äësize model (‚â§‚ÄØ30‚ÄØB parameters)\n","\n","Choose **one or more** of the options below:\n","\n","**A.¬†Hugging¬†Face checkpoint** (local inference) ‚Äì already configured in the code cell that follows.\n","\n","**B.¬†OpenAI model** ‚Äì call an OpenAI hosted model (e.g. `gpt‚Äë4o`, `gpt‚Äë4‚Äëturbo`, `gpt‚Äë3.5‚Äëturbo`). Implement `call_openai(prompt: str) -> str` in a separate utility cell and then run the snippet.\n","\n","**C.¬†Google Gemini model** ‚Äì call a Gemini endpoint (e.g. `gemini‚Äë1.5‚Äëpro`). Implement `call_gemini(prompt: str) -> str` similarly.\n","\n","> ‚ö†Ô∏è‚ÄØMake sure you have your API keys set as environment variables or passed securely.\n","\n","\n","**Latency & cost tracking**\n","- Your `call_*` functions should return a **dict** with keys:\n","  - `text` ‚Äì generated description (string)\n","  - `latency_ms` ‚Äì end‚Äëto‚Äëend generation time in milliseconds\n","  - `input_tokens` ‚Äì tokens sent to the model (**IF YOU ADDED A SYS PROMPT ADD IT TO THE CALCULATION**)\n","  - `output_tokens` ‚Äì tokens received from the model\n","- Below, a helper `call_hf()` shows how to compute these metrics for a Hugging Face model. You must add equivalent tracking inside `call_openai()` and `call_gemini()`.\n"]},{"cell_type":"markdown","id":"65e10504","metadata":{"id":"65e10504"},"source":["**FOR COLAB USERS**\n","\n","You can set your HF_TOKEN secret in Colab, please follow these steps:\n","\n","1. Click on the \"üîë\" icon in the left-hand sidebar in Colab. This opens the Secrets manager.\n","2. Click on \"New secret\".\n","3. In the \"Name\" field, type HF_TOKEN.\n","4. In the \"Value\" field, paste your Hugging Face access token (you can generate one from your Hugging Face account settings under \"Access Tokens\").\n","5. Make sure the \"Notebook access\" toggle is enabled for your notebook.\n","6. Close the Secrets manager."]},{"cell_type":"code","execution_count":5,"id":"2fda2420","metadata":{"id":"2fda2420"},"outputs":[{"name":"stdout","output_type":"stream","text":["YOUR_HF_TOKEN\n"]}],"source":["# Set your Hugging Face access token\n","import os\n","os.environ[\"HF_TOKEN\"] = \"YOUR_HF_TOKEN\"\n","print(os.environ.get(\"HF_TOKEN\"))"]},{"cell_type":"code","execution_count":6,"id":"2ad5f6c8","metadata":{"id":"2ad5f6c8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/kimchen/anaconda3/envs/new-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"ename":"AssertionError","evalue":"Switch Colab to GPU first","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSwitch Colab to GPU first\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m bnb \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mAssertionError\u001b[0m: Switch Colab to GPU first"]}],"source":["import torch, os\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n","\n","assert torch.cuda.is_available(), \"Switch Colab to GPU first\"\n","\n","model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","\n","bnb = BitsAndBytesConfig(load_in_4bit=True)\n","\n","# this will work with L4 GPU, if you have a different GPU, you may need to modify the code\n","hf_model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n","    max_memory={0: \"24GiB\", \"cpu\": \"30GiB\"},\n","    quantization_config=bnb,\n","    low_cpu_mem_usage=True\n",")\n","\n","hf_tok = AutoTokenizer.from_pretrained(model_id)\n","\n","hf_gen = pipeline(\"text-generation\",\n","               model=hf_model,\n","               tokenizer=hf_tok,\n","               max_new_tokens=120,\n","               do_sample=False)"]},{"cell_type":"code","execution_count":7,"id":"314d9fff","metadata":{"id":"314d9fff"},"outputs":[{"ename":"NameError","evalue":"name 'hf_gen' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m      2\u001b[0m     product_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEverCool Water Bottle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     Product_attribute_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdouble-wall vacuum insulation; keeps drinks cold 24 h; leak-proof lid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     material\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstainless steel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     warranty\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlifetime warranty\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mhf_gen\u001b[49m(prompt_tmpl\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msample),\n\u001b[1;32m      9\u001b[0m           return_full_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n","\u001b[0;31mNameError\u001b[0m: name 'hf_gen' is not defined"]}],"source":["sample = dict(\n","    product_name=\"EverCool Water Bottle\",\n","    Product_attribute_list=\"double-wall vacuum insulation; keeps drinks cold 24 h; leak-proof lid\",\n","    material=\"stainless steel\",\n","    warranty=\"lifetime warranty\",\n",")\n","\n","print(hf_gen(prompt_tmpl.format(**sample),\n","          return_full_text=False)[0][\"generated_text\"])"]},{"cell_type":"code","execution_count":null,"id":"ef0e2a14","metadata":{"id":"ef0e2a14"},"outputs":[],"source":["# --- Option¬†A: Hugging Face ---\n","# Ensure you implemented call_hf() in another cell (you can use the one from the previous assignment).\n","\n","# Example with HF pipeline\n","def call_hf(prompt: str, model_id: str = \"mistralai/Mistral-7B-Instruct-v0.3\"):\n","    \"\"\"Return dict with latency & token counts for HF pipeline.\"\"\"\n","    # tokenizer = AutoTokenizer.from_pretrained(model_id)\n","    # model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_8bit=True)\n","    # generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=120)\n","\n","    import time\n","    start = time.time()\n","    res= hf_gen(prompt,\n","          return_full_text=False)[0][\"generated_text\"]\n","\n","    latency = (time.time() - start) * 1000  # ms\n","    # token counts via tokenizer\n","    input_tokens = len(hf_tok.encode(prompt))\n","    output_tokens = len(hf_tok.encode(res))\n","    return {\n","        \"text\": res,\n","        \"latency_ms\": latency,\n","        \"input_tokens\": input_tokens,\n","        \"output_tokens\": output_tokens,\n","    }\n","\n","response_hf = call_hf(prompt_tmpl.format(**sample))\n","print(response_hf)"]},{"cell_type":"code","execution_count":null,"id":"0b468a65","metadata":{"id":"0b468a65"},"outputs":[],"source":["# --- Option¬†B: OpenAI ---\n","# Ensure you implemented call_openai() in another cell (you can use the one from the previous assignment).\n","# from your_utils import call_openai  # example import if needed\n","# response_oai = call_openai(prompt, model_name=\"gpt-4o\")\n","# print(response_oai)"]},{"cell_type":"code","execution_count":8,"id":"8e8efaf2","metadata":{},"outputs":[],"source":["import google.generativeai as genai\n","import time"]},{"cell_type":"code","execution_count":9,"id":"307bd09d","metadata":{},"outputs":[],"source":["from dotenv import load_dotenv\n","import os\n","\n","# Load the .env file\n","load_dotenv()\n","\n","# Access variables\n","gemini_api_key = os.getenv(\"gemini_api_key\")"]},{"cell_type":"code","execution_count":null,"id":"3889b100","metadata":{},"outputs":[],"source":["import openai\n","import time\n","\n","def call_openai(prompt: str, model_name: str = \"gpt-4o\", api_key: str = None) -> dict:\n","    \"\"\"\n","    Call OpenAI API and return generated text and usage metrics.\n","\n","    Args:\n","        prompt (str): The user prompt.\n","        model_name (str): OpenAI model to use (default: \"gpt-4o\").\n","        api_key (str): OpenAI API Key (optional if already set globally).\n","\n","    Returns:\n","        dict: {\n","            \"text\": generated description (str),\n","            \"latency_ms\": latency in milliseconds (float),\n","            \"input_tokens\": tokens sent (int),\n","            \"output_tokens\": tokens received (int)\n","        }\n","    \"\"\"\n","    if api_key:\n","        openai.api_key = api_key\n","\n","    start_time = time.time()\n","\n","    try:\n","        response = openai.ChatCompletion.create(\n","            model=model_name,\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","            temperature=0.7\n","        )\n","\n","        end_time = time.time()\n","        latency_ms = (end_time - start_time) * 1000\n","\n","        output = response.choices[0].message.content.strip()\n","        usage = response.usage\n","\n","        return {\n","            \"text\": output,\n","            \"latency_ms\": latency_ms,\n","            \"input_tokens\": usage.prompt_tokens if usage else None,\n","            \"output_tokens\": usage.completion_tokens if usage else None\n","        }\n","\n","    except Exception as e:\n","        print(f\"OpenAI API Error: {e}\")\n","        return {\n","            \"text\": \"\",\n","            \"latency_ms\": 0,\n","            \"input_tokens\": 0,\n","            \"output_tokens\": 0\n","        }\n"]},{"cell_type":"code","execution_count":10,"id":"0db18322","metadata":{"id":"0db18322"},"outputs":[],"source":["# --- Option¬†C: Gemini ---\n","# --- Option¬†C: Gemini ---\n","def call_gemini(prompt: str, model_name: str = \"models/gemini-1.5-pro\", api_key: str = gemini_api_key) -> dict:\n","    \"\"\"\n","    Call Google Gemini API and return generated text and usage metrics.\n","\n","    Args:\n","        prompt (str): The user prompt.\n","        model_name (str): Gemini model to use (default: \"gemini-pro\").\n","        api_key (str): Google API Key.\n","\n","    Returns:\n","        dict: {\n","            \"text\": generated description (str),\n","            \"latency_ms\": latency in milliseconds (float),\n","            \"input_tokens\": tokens sent (int),\n","            \"output_tokens\": tokens received (int)\n","        }\n","    \"\"\"\n","\n","    if not api_key:\n","        raise ValueError(\"api_key must be provided for Google Gemini API\")\n","\n","    genai.configure(api_key=api_key)\n","\n","    # Initialize model\n","    model = genai.GenerativeModel(model_name)\n","\n","    # Measure start time\n","    start_time = time.time()\n","\n","    try:\n","        response = model.generate_content(prompt)\n","        end_time = time.time()\n","        latency_ms = (end_time - start_time) * 1000\n","\n","        text = response.text.strip() if hasattr(response, \"text\") else \"\"\n","\n","        # Token accounting (Gemini SDK does not expose usage directly yet)\n","        input_tokens = len(prompt.split())\n","        output_tokens = len(text.split())\n","\n","        return {\n","            \"text\": text,\n","            \"latency_ms\": latency_ms,\n","            \"input_tokens\": input_tokens,\n","            \"output_tokens\": output_tokens\n","        }\n","\n","    except Exception as e:\n","        print(f\"Gemini API Error: {e}\")\n","        return {\n","            \"text\": \"\",\n","            \"latency_ms\": 0,\n","            \"input_tokens\": 0,\n","            \"output_tokens\": 0\n","        }"]},{"cell_type":"code","execution_count":15,"id":"554467dd","metadata":{"id":"554467dd"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>product_name</th>\n","      <th>Product_attribute_list</th>\n","      <th>material</th>\n","      <th>warranty</th>\n","      <th>generated_description</th>\n","      <th>latency_ms</th>\n","      <th>input_tokens</th>\n","      <th>output_tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Apple iPhone 15 Pro</td>\n","      <td>features: A17 Pro chip, 120‚ÄØHz ProMotion displ...</td>\n","      <td>titanium frame, Ceramic Shield glass</td>\n","      <td>1‚Äëyear limited warranty</td>\n","      <td>Experience the blazing-fast speed of the Apple...</td>\n","      <td>2322.148800</td>\n","      <td>49</td>\n","      <td>59</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Samsung Galaxy S24 Ultra</td>\n","      <td>features: 200‚ÄØMP camera, S‚ÄëPen support, 120‚ÄØHz...</td>\n","      <td>Armor Aluminum frame, Gorilla Glass Victus</td>\n","      <td>1‚Äëyear limited warranty</td>\n","      <td>Capture brilliance with the Samsung Galaxy S24...</td>\n","      <td>2191.933155</td>\n","      <td>48</td>\n","      <td>57</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Google Pixel 8 Pro</td>\n","      <td>features: Tensor G3 chip, Magic Eraser, 50‚ÄØMP ...</td>\n","      <td>matte glass back, aluminum frame</td>\n","      <td>1‚Äëyear limited warranty</td>\n","      <td>Experience the power of the Google Pixel 8 Pro...</td>\n","      <td>2248.890877</td>\n","      <td>47</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sony WH‚Äë1000XM5 Headphones</td>\n","      <td>features: active noise cancelling, 30‚ÄØhr batte...</td>\n","      <td>synthetic leather earcups</td>\n","      <td>1‚Äëyear limited warranty</td>\n","      <td>Experience sound like never before with the So...</td>\n","      <td>2106.420040</td>\n","      <td>44</td>\n","      <td>54</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Bose QuietComfort Ultra Earbuds</td>\n","      <td>features: CustomTune sound calibration, ANC, I...</td>\n","      <td>silicone ear tips</td>\n","      <td>1‚Äëyear limited warranty</td>\n","      <td>Experience world-class sound and silence with ...</td>\n","      <td>1988.033056</td>\n","      <td>42</td>\n","      <td>54</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                      product_name  \\\n","0              Apple iPhone 15 Pro   \n","1         Samsung Galaxy S24 Ultra   \n","2               Google Pixel 8 Pro   \n","3       Sony WH‚Äë1000XM5 Headphones   \n","4  Bose QuietComfort Ultra Earbuds   \n","\n","                              Product_attribute_list  \\\n","0  features: A17 Pro chip, 120‚ÄØHz ProMotion displ...   \n","1  features: 200‚ÄØMP camera, S‚ÄëPen support, 120‚ÄØHz...   \n","2  features: Tensor G3 chip, Magic Eraser, 50‚ÄØMP ...   \n","3  features: active noise cancelling, 30‚ÄØhr batte...   \n","4  features: CustomTune sound calibration, ANC, I...   \n","\n","                                     material                 warranty  \\\n","0        titanium frame, Ceramic Shield glass  1‚Äëyear limited warranty   \n","1  Armor Aluminum frame, Gorilla Glass Victus  1‚Äëyear limited warranty   \n","2            matte glass back, aluminum frame  1‚Äëyear limited warranty   \n","3                   synthetic leather earcups  1‚Äëyear limited warranty   \n","4                           silicone ear tips  1‚Äëyear limited warranty   \n","\n","                               generated_description   latency_ms  \\\n","0  Experience the blazing-fast speed of the Apple...  2322.148800   \n","1  Capture brilliance with the Samsung Galaxy S24...  2191.933155   \n","2  Experience the power of the Google Pixel 8 Pro...  2248.890877   \n","3  Experience sound like never before with the So...  2106.420040   \n","4  Experience world-class sound and silence with ...  1988.033056   \n","\n","   input_tokens  output_tokens  \n","0            49             59  \n","1            48             57  \n","2            47             60  \n","3            44             54  \n","4            42             54  "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# --- Batch generation helper (type‚Äësafe) ---\n","from typing import Callable\n","import pandas as pd\n","\n","from typing import Callable, Dict\n","import pandas as pd\n","\n","def batch_generate(\n","    sample_df: pd.DataFrame,\n","    call_model_fn: Callable[[str], Dict[str, object]],\n","    prompt_template: str = prompt_tmpl,\n",") -> pd.DataFrame:\n","    \"\"\"Generate descriptions and metrics for each row in *sample_df*.\n","\n","    The model-calling function *must* return a dict with keys:\n","    - ``text`` (str) ‚Äì generated description\n","    - ``latency_ms`` (float | None)\n","    - ``input_tokens`` (int | None)\n","    - ``output_tokens`` (int | None)\n","    \"\"\"\n","    if not isinstance(sample_df, pd.DataFrame):\n","        raise TypeError(\"sample_df must be a pandas DataFrame\")\n","    if not callable(call_model_fn):\n","        raise TypeError(\"call_model_fn must be callable\")\n","\n","    outputs = []\n","    for _, row in sample_df.iterrows():\n","        prompt = prompt_template.format(**row.to_dict())\n","        out = call_model_fn(prompt)\n","        if not isinstance(out, dict) or 'text' not in out:\n","            raise ValueError(\"call_model_fn must return a dict with at least a 'text' field\")\n","        outputs.append(out)\n","\n","    result_df = sample_df.copy()\n","    result_df[\"generated_description\"] = [o[\"text\"] for o in outputs]\n","    result_df[\"latency_ms\"] = [o.get(\"latency_ms\") for o in outputs]\n","    result_df[\"input_tokens\"] = [o.get(\"input_tokens\") for o in outputs]\n","    result_df[\"output_tokens\"] = [o.get(\"output_tokens\") for o in outputs]\n","    return result_df\n","\n","\n","demo_df = batch_generate(df_products[:5], call_gemini)\n","demo_df.head()"]},{"cell_type":"markdown","id":"e25cb4ba","metadata":{"id":"e25cb4ba"},"source":["## 6‚ÄØ¬†Manual evaluation\n","Use `batch_generate()` to create a DataFrame of model outputs, then add blank rating columns for each criterion plus a `final_score` column. An Excel file is saved so you can fill scores offline or share with peers.\n","\n","Steps:\n","1. Run the code cell below (adjust which `call_*` function you pass in).\n","2. Open the generated `assignment_03_evaluation_sheet.xlsx` and rate each row with **good / ok / bad**.\n","3. Add a rule for `final_score` (e.g., majority = good, fails if grounding = bad).\n","\n","\n","**Cost calculator**\n","Use the helper below to compute cost in USD based on token usage:\n","```python\n","outputs_df = add_cost_columns(outputs_df, input_price_per_m=1.5, output_price_per_m=2.0)\n","```\n","Set prices to **0** if you ran everything locally on Hugging Face."]},{"cell_type":"code","execution_count":18,"id":"451b027b","metadata":{"id":"451b027b"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>product_name</th>\n","      <th>Product_attribute_list</th>\n","      <th>material</th>\n","      <th>warranty</th>\n","      <th>generated_description</th>\n","      <th>latency_ms</th>\n","      <th>input_tokens</th>\n","      <th>output_tokens</th>\n","      <th>cost_usd</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Apple iPhone 15 Pro</td>\n","      <td>features: A17 Pro chip, 120‚ÄØHz ProMotion displ...</td>\n","      <td>titanium frame, Ceramic Shield glass</td>\n","      <td>1‚Äëyear limited warranty</td>\n","      <td>Experience the blazing-fast speed of the Apple...</td>\n","      <td>2322.148800</td>\n","      <td>49</td>\n","      <td>59</td>\n","      <td>0.0002</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Samsung Galaxy S24 Ultra</td>\n","      <td>features: 200‚ÄØMP camera, S‚ÄëPen support, 120‚ÄØHz...</td>\n","      <td>Armor Aluminum frame, Gorilla Glass Victus</td>\n","      <td>1‚Äëyear limited warranty</td>\n","      <td>Capture brilliance with the Samsung Galaxy S24...</td>\n","      <td>2191.933155</td>\n","      <td>48</td>\n","      <td>57</td>\n","      <td>0.0002</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Google Pixel 8 Pro</td>\n","      <td>features: Tensor G3 chip, Magic Eraser, 50‚ÄØMP ...</td>\n","      <td>matte glass back, aluminum frame</td>\n","      <td>1‚Äëyear limited warranty</td>\n","      <td>Experience the power of the Google Pixel 8 Pro...</td>\n","      <td>2248.890877</td>\n","      <td>47</td>\n","      <td>60</td>\n","      <td>0.0002</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sony WH‚Äë1000XM5 Headphones</td>\n","      <td>features: active noise cancelling, 30‚ÄØhr batte...</td>\n","      <td>synthetic leather earcups</td>\n","      <td>1‚Äëyear limited warranty</td>\n","      <td>Experience sound like never before with the So...</td>\n","      <td>2106.420040</td>\n","      <td>44</td>\n","      <td>54</td>\n","      <td>0.0002</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Bose QuietComfort Ultra Earbuds</td>\n","      <td>features: CustomTune sound calibration, ANC, I...</td>\n","      <td>silicone ear tips</td>\n","      <td>1‚Äëyear limited warranty</td>\n","      <td>Experience world-class sound and silence with ...</td>\n","      <td>1988.033056</td>\n","      <td>42</td>\n","      <td>54</td>\n","      <td>0.0002</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                      product_name  \\\n","0              Apple iPhone 15 Pro   \n","1         Samsung Galaxy S24 Ultra   \n","2               Google Pixel 8 Pro   \n","3       Sony WH‚Äë1000XM5 Headphones   \n","4  Bose QuietComfort Ultra Earbuds   \n","\n","                              Product_attribute_list  \\\n","0  features: A17 Pro chip, 120‚ÄØHz ProMotion displ...   \n","1  features: 200‚ÄØMP camera, S‚ÄëPen support, 120‚ÄØHz...   \n","2  features: Tensor G3 chip, Magic Eraser, 50‚ÄØMP ...   \n","3  features: active noise cancelling, 30‚ÄØhr batte...   \n","4  features: CustomTune sound calibration, ANC, I...   \n","\n","                                     material                 warranty  \\\n","0        titanium frame, Ceramic Shield glass  1‚Äëyear limited warranty   \n","1  Armor Aluminum frame, Gorilla Glass Victus  1‚Äëyear limited warranty   \n","2            matte glass back, aluminum frame  1‚Äëyear limited warranty   \n","3                   synthetic leather earcups  1‚Äëyear limited warranty   \n","4                           silicone ear tips  1‚Äëyear limited warranty   \n","\n","                               generated_description   latency_ms  \\\n","0  Experience the blazing-fast speed of the Apple...  2322.148800   \n","1  Capture brilliance with the Samsung Galaxy S24...  2191.933155   \n","2  Experience the power of the Google Pixel 8 Pro...  2248.890877   \n","3  Experience sound like never before with the So...  2106.420040   \n","4  Experience world-class sound and silence with ...  1988.033056   \n","\n","   input_tokens  output_tokens  cost_usd  \n","0            49             59    0.0002  \n","1            48             57    0.0002  \n","2            47             60    0.0002  \n","3            44             54    0.0002  \n","4            42             54    0.0002  "]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# --- Cost computation helper ---\n","def add_cost_columns(df, input_price_per_m: float, output_price_per_m: float):\n","    \"\"\"Add cost columns based on token counts.\n","    Args:\n","        df: DataFrame with `input_tokens` and `output_tokens`.\n","        input_price_per_m: $ per 1M input tokens.\n","        output_price_per_m: $ per 1M output tokens.\n","    Returns: DataFrame with extra `cost_usd` column.\n","    \"\"\"\n","    if 'input_tokens' not in df or 'output_tokens' not in df:\n","        raise ValueError('Token columns missing; run batch_generate first')\n","    cost_input = df['input_tokens'] * (input_price_per_m / 1000000)\n","    cost_output = df['output_tokens'] * (output_price_per_m / 1000000)\n","    df = df.copy()\n","    df['cost_usd'] = (cost_input + cost_output).round(4)\n","    return df\n","\n","# Example usage (set prices to 0 for HF local models):\n","outputs_df = add_cost_columns(demo_df, input_price_per_m=1.5, output_price_per_m=2.0)\n","outputs_df.head()"]},{"cell_type":"code","execution_count":20,"id":"4386c3f5","metadata":{"id":"4386c3f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Saved evaluation sheet ‚Üí evaluated_assignment_03.xlsx with 50 rows\n"]}],"source":["# --- Build evaluation sheet & export to Excel ---\n","\n","#Update the prices according to the model you used, or leave them at 0 for HF local models\n","YOUR_MODEL_INPUT_PRICE_PER_M = 1.25\n","YOUR_MODEL_OUTPUT_PRICE_PER_M = 5\n","outputs_df = batch_generate(df_products, call_gemini)  # NOTE: change model function as needed\n","\n","# Add rating columns (good/ok/bad)\n","rating_cols = [\"fluency\", \"grammar\", \"tone\", \"length\", \"grounding\", \"latency\", \"cost\", \"final_score\"]\n","for col in rating_cols:\n","    if col not in outputs_df:\n","        outputs_df[col] = \"\"\n","\n","xlsx_path = \"evaluated_assignment_03.xlsx\"\n","\n","# Add cost columns\n","outputs_df = add_cost_columns(outputs_df, YOUR_MODEL_INPUT_PRICE_PER_M, YOUR_MODEL_OUTPUT_PRICE_PER_M)\n","\n","outputs_df.to_excel(xlsx_path, index=False)\n","print(f\"Saved evaluation sheet ‚Üí {xlsx_path} with {len(outputs_df)} rows\")"]},{"cell_type":"markdown","id":"38dcbe33","metadata":{"id":"38dcbe33"},"source":["## 7‚ÄØ¬†Improvement cycle\n","\n","Now that you‚Äôve established a baseline score in **Section‚ÄØ6**, iterate to achieve better results.\n","\n","**Ideas to explore**\n","- **Prompt tuning**¬†‚Äì rewrite the system/user prompts, add few‚Äëshot examples, or enforce stricter constraints.\n","- **Model choice**¬†‚Äì test a different checkpoint (larger ‚â† always better), switch from HF to OpenAI or Gemini, or try a domain‚Äëspecific model.\n","- **Temperature / decoding params**¬†‚Äì adjust `temperature`, `top_p`, `top_k`, or `max_new_tokens` to balance creativity vs. factuality.\n","- **Data preprocessing**¬†‚Äì clean attribute text, expand abbreviations, or group similar products to feed additional context.\n","- **Post‚Äëprocessing**¬†‚Äì run grammar‚Äëchecking or length trimming after generation.\n","- **Ensembling / RAG**¬†‚Äì combine outputs from two models or ground the prompt with retrieved copy from existing catalog listings.\n","\n","Document each experiment in a brief bullet list:\n","1. **What you changed**\n","2. **Why you expected it to help**\n","3. **New evaluation scores**\n","\n","üí° *Goal*: maximize the cumulative score according to your rubric while respecting the go/no‚Äëgo rules."]},{"cell_type":"markdown","id":"0534c68b","metadata":{"id":"0534c68b"},"source":["# Part‚ÄØ2¬†‚Äì Judging¬†Language¬†Models¬†(JLMs)"]},{"cell_type":"markdown","id":"045d584e","metadata":{"id":"045d584e"},"source":["In Part¬†1 we generated model outputs and evaluated the outputs according to a set of business criterion.\n","Now we will build **JLMs** that (try really hard to match what you did and) automatically *grade* those outputs."]},{"cell_type":"markdown","id":"d5ff34e2","metadata":{"id":"d5ff34e2"},"source":["## 1¬†Na√Øve¬†JLM¬†(same model as generation)\n","\n","For the product‚Äëdescription task there is **no single reference answer**.  \n","Your na√Øve JLM should simply *critique the description itself* using the same\n","LLM that produced it.\n","\n","* Inputs: the generated product description (string) and any rubric bullets you\n","  think matter (e.g. factual accuracy, persuasiveness, tone).  \n","* Output: **any useful signal** ‚Äì free‚Äëtext comments, a JSON blob, a (score, notes) tuple‚Ä¶ your choice.\n","\n","> **Deliverable:** implement `naive_jlm(description)` that calls the LLM once and\n","> returns its raw judgment output."]},{"cell_type":"code","execution_count":null,"id":"46657a13","metadata":{},"outputs":[],"source":["judge_prompt_tmpl =\"\"\"\n","You are an expert evaluator of marketing copy. Based on the product information and the generated description below, evaluate the description using **only** these labels: \"good\", \"ok\", or \"bad\".\n","\n","Rate each of the following categories:\n","- fluency\n","- grammar\n","- tone\n","- length (should be between 50‚Äì90 words)\n","- grounding (faithfulness to product attributes)\n","- latency (perceived delivery speed / verbosity)\n","- cost (efficiency of wording)\n","- final_score (overall quality)\n","\n","**Product Details**\n","- Product name: {product_name}\n","- Key features: {Product_attribute_list}\n","- Material: {material}\n","- Warranty: {warranty}\n","\n","**Generated Description**\n","{generated_description}\n","\n","Return your evaluation in the following JSON format:\n","\n","{{\n","  \"fluency\": \"<good|ok|bad>\",\n","  \"grammar\": \"<good|ok|bad>\",\n","  \"tone\": \"<good|ok|bad>\",\n","  \"length\": \"<good|ok|bad>\",\n","  \"grounding\": \"<good|ok|bad>\",\n","  \"latency\": \"<good|ok|bad>\",\n","  \"cost\": \"<good|ok|bad>\",\n","  \"final_score\": \"<good|ok|bad>\"\n","}}\n","\"\"\"\n"]},{"cell_type":"code","execution_count":22,"id":"9f3d6fdd","metadata":{},"outputs":[],"source":["description = \"Experience the pinnacle of innovation with the Apple iPhone 15 Pro.  Its powerful A17 Pro chip fuels a breathtaking 120Hz ProMotion display for incredibly smooth visuals.  The durable titanium frame and Ceramic Shield glass encase this compact powerhouse. Enjoy blazing-fast charging with USB-C. Comes with a 1-year limited warranty for peace of mind. Upgrade to pro.\""]},{"cell_type":"code","execution_count":23,"id":"6725212c","metadata":{"id":"6725212c"},"outputs":[{"data":{"text/plain":["{'text': '```json\\n{\\n  \"strengths\": [\\n    \"Highlights key features like the A17 Pro chip, ProMotion display, and durable construction.\",\\n    \"Uses evocative language such as \\'breathtaking,\\' \\'incredibly smooth,\\' and \\'blazing-fast.\\'\",\\n    \"Includes practical information like the USB-C charging and warranty.\",\\n    \"Clear call to action: \\'Upgrade to pro.\\'\"\\n  ],\\n  \"weaknesses\": [\\n    \"Lacks specific details about camera specs, storage options, and other important features.\",\\n    \"Overuses superlatives like \\'pinnacle\\' and \\'powerful\\' without providing quantifiable support.\",\\n    \"The phrase \\'compact powerhouse\\' could be contradictory and depends on the user\\'s perspective.\",\\n    \"Doesn\\'t mention the price, which is a crucial factor for purchase decisions.\"\\n  ],\\n  \"rating\": 3\\n}\\n```',\n"," 'latency_ms': 4060.133934020996,\n"," 'input_tokens': 83,\n"," 'output_tokens': 98}"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# ------------------------- STUDENT TODO -------------------------\n","# Implement a simple self‚Äëcritique using the SAME model used for generation.\n","# No reference answer is available, so evaluate the description directly.\n","\n","# IMPORTANT NOTE :use the same model as the one you used for generation\n","def naive_jlm(description: str):\n","    \"\"\"Return a naive judgment (format of your choice) for a product description.\"\"\"\n","    # Example pseudocode:\n","    prompt = \"\"\"You are a product‚Äëdescription critic. Evaluate the following description\n","     on clarity, appeal, and accuracy. Return JSON with 'strengths', 'weaknesses',\n","     and an overall 'rating' from 1‚Äë5.\\n\\nDescription:\\n{description}\\nJSON:\"\"\"\n","    \n","    response = call_gemini(prompt.format(description=description))\n","    return response\n","\n","naive_jlm(description)"]},{"cell_type":"markdown","id":"466a9eb1","metadata":{"id":"466a9eb1"},"source":["## 2¬†Structured¬†JLM¬†‚Äì JSON per criterion\n","\n","We want our grader to return **machine‚Äëreadable JSON** so that we can later **automate\n","aggregate scoring and analysis** (e.g., Section‚ÄØ5 majority vote, dashboards, or batch\n","experiments).\n","\n","For every evaluation criterion you defined in Part¬†1¬†sectoin 3, output an object that looks\n","like this (keys must match your own criteria list):\n","\n","* **Fluency**  \n","* **Grammar**  \n","* **Tone**  \n","* **Length**  \n","* **Grounding**¬†¬†(e.g. factual alignment / no hallucinations)\n","\n","We will grade every description on the **criteria** we care about for this\n","assignment:\n","```json\n","{\n","  \"Fluency\":   {\"explanation\": \"...\", \"verdict\": \"good\"},\n","  \"Grammar\":   {\"explanation\": \"...\", \"verdict\": \"ok\"},\n","  \"Tone\":      {\"explanation\": \"...\", \"verdict\": \"good\"},\n","  \"Length\":    {\"explanation\": \"...\", \"verdict\": \"good\"},\n","  \"Grounding\": {\"explanation\": \"...\", \"verdict\": \"good\"},\n","}\n","```\n","\n","\n","*\\*The following will be ignored for convinence and as they do not require JLM*\n","* **Latency**\n","* **Cost**\n","\n","\n","\n","\n","### Why `{explanation, verdict}`?\n","\n","1. **Justification before decision** ‚Äì forcing the model to produce an *explanation*\n","   **first** encourages reasoned thinking; if it wrote the verdict first, the\n","   explanation might simply try to defend that label (confirmation bias).  \n","2. **Auditability** ‚Äì having a short explanation lets us *spot‚Äëcheck* the grader\n","   for hallucinations or mis‚Äëinterpretations down the line.  \n","3. **Closed‚Äëclass verdicts** ‚Äì restricting the verdict to a small set\n","   (`good‚ÄØ|‚ÄØok‚ÄØ|‚ÄØbad`) enables fast, deterministic score calculations in later\n","   automated pipelines.\n","\n","### Your implementation\n","\n","* **Backend choice:** `call_openai()`, `call_gemini()`, `call_hf()`, or a\n","  LangChain chat model ‚Äì choose whatever you prefer.  \n","* **Prompt freedom:** use the template we provide *or rewrite it entirely*.  \n","* Loop over the global `criteria` list from Part¬†1¬†¬ß3.  \n","* Implement **`structured_jlm(description)`** so the cell runs without errors and\n","  returns the parsed JSON."]},{"cell_type":"code","execution_count":24,"id":"adf819d3","metadata":{"id":"adf819d3"},"outputs":[{"ename":"TypeError","evalue":"the JSON object must be str, bytes or bytearray, not dict","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Invalid JSON response from model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mstructured_jlm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/gemini-1.5-pro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[24], line 38\u001b[0m, in \u001b[0;36mstructured_jlm\u001b[0;34m(description, model_name)\u001b[0m\n\u001b[1;32m     35\u001b[0m raw \u001b[38;5;241m=\u001b[39m call_model(prompt, model_name) \u001b[38;5;66;03m# NOTICE this is the raw response, modify it as you will to make the next step work\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;66;03m#NOTE: Making sure your model returns a valid JSON object is sometimes harder than you think...\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Invalid JSON response from model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m~/anaconda3/envs/new-env/lib/python3.12/json/__init__.py:339\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[0;32m--> 339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    340\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n","\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not dict"]}],"source":["# ------------------------- STUDENT TODO -------------------------\n","import json\n","\n","# Fixed criteria list for the assignment\n","criteria = [\"Fluency\", \"Grammar\", \"Tone\", \"Length\", \"Grounding\"]\n","\n","# Choose backend: \"openai\", \"gemini\", \"hf\"\n","BACKEND = \"gemini\"\n","\n","# Prompt template (edit freely)\n","PROMPT_TEMPLATE = \"\"\"You are a product‚Äëdescription critic.\n","For each criterion in the list below, provide a JSON object with:\n","  ‚Ä¢ explanation (1‚Äë2 sentences)\n","  ‚Ä¢ verdict      (good | ok | bad)\n","\n","Criteria: {criteria}\n","\n","Description:\n","{description}\n","\n","JSON:\"\"\"\n","\n","#IMPORTANT NOTE :use the same model as the one you used for generation\n","def call_model(prompt: str, model_name: str) -> str:\n","    if BACKEND == \"openai\":\n","        return call_openai(prompt, model_name)\n","    elif BACKEND == \"gemini\":\n","        return call_gemini(prompt, model_name)\n","    elif BACKEND == \"hf\":\n","        return call_hf(prompt, model_name)\n","\n","def structured_jlm(description: str, model_name:str) -> dict:\n","    prompt = PROMPT_TEMPLATE.format(criteria=\", \".join(criteria),\n","                                    description=description)\n","    raw = call_model(prompt, model_name) # NOTICE this is the raw response, modify it as you will to make the next step work\n","\n","    try: #NOTE: Making sure your model returns a valid JSON object is sometimes harder than you think...\n","        return json.loads(raw)\n","    except json.JSONDecodeError:\n","        raise ValueError(f\"Error: Invalid JSON response from model: {raw}\")\n","\n","# ---------------------------------------------------------------\n","\n","structured_jlm(description, model_name=\"models/gemini-1.5-pro\")"]},{"cell_type":"markdown","id":"92fb2b21","metadata":{"id":"92fb2b21"},"source":["## 3¬†Criterion‚Äëby‚ÄëCriterion¬†Calls\n","\n","Instead of asking the JLM to grade *all* criteria in one big prompt, we can\n","make **one call per criterion** and merge the results.\n","\n","### Why split?\n","\n","1. **Sharper focus¬†‚á¢ better judgments**  \n","   The model concentrates on a *single* dimension at a time, which often boosts\n","   accuracy ‚Äì atomic tasks are easier.  \n","2. **Richer, criterion‚Äëspecific instructions**  \n","   You can craft a longer, more detailed prompt for *each* criterion without\n","   squeezing everything into one context window.\n","\n","### Trade‚Äëoffs\n","\n","1. **Higher cost** ‚Äì `#criteria √ó price‚Äëper‚Äëcall` can add up.  \n","2. **Higher latency** ‚Äì multiple sequential calls take longer than one.\n","\n","\n","\n","### Your task\n","\n","1. Implement **`per_criterion_jlm(description)`** (see code cell below) using any backend (`call_openai`, `call_gemini`, `call_hf`, or `jlm_llm`).\n","2. **Pick one generated product description** and run **all three graders**:\n","   * `naive_jlm(description)` (Section¬†1)\n","   * `structured_jlm(description)` (Section¬†2)\n","   * `per_criterion_jlm(description)` (this section)\n","3. Compare the outputs. Note any differences in explanations and verdicts.\n","\n","Write two‚Äëthree sentences of reflection in the markdown cell that follows: *Which approach seems most trustworthy for this task and why?*"]},{"cell_type":"code","execution_count":null,"id":"a75867ae","metadata":{"id":"a75867ae"},"outputs":[],"source":["# ------------------------- STUDENT TODO -------------------------\n","import json\n","\n","criteria = [\"Fluency\", \"Grammar\", \"Tone\", \"Length\", \"Grounding\"]\n","\n","# Re‚Äëuse BACKEND and call_model() from Section¬†2, or redefine here.\n","try:\n","    BACKEND\n","except NameError:\n","    BACKEND = \"hf\"  # fallback; set to openai / gemini / hf as needed\n","\n","# If call_model is not defined (e.g., you skipped Section‚ÄØ2), define a minimal version\n","if \"call_model\" not in globals():\n","    def call_model(prompt: str, model_name: str) -> str:\n","        if BACKEND == \"openai\":\n","            return call_openai(prompt, model_name)\n","        elif BACKEND == \"gemini\":\n","            return call_gemini(prompt, model_name)\n","        elif BACKEND == \"hf\":\n","            return call_hf(prompt, model_name)\n","\n","# Template per criterion (modify as you like)\n","CRIT_PROMPT_TEMPLATE = \"\"\"You are a product‚Äëdescription critic.\n","Criterion: {criterion}\n","Return JSON with keys 'explanation' and 'verdict' (good|ok|bad).\n","\n","Description:\n","{description}\n","\n","JSON:\"\"\"\n","\n","#IMPORTANT NOTE :use the same model as the one you used for generation\n","def per_criterion_jlm(description: str, model_name: str, criteria: list[str] | None = None) -> dict:\n","    \"\"\"Grade *description* against each criterion in *criteria* with separate LLM calls.\n","\n","    Parameters\n","    ----------\n","    description : str\n","        Product description to evaluate.\n","    criteria : list[str], optional\n","        List of evaluation criteria. If *None*, falls back to the global ``criteria`` list\n","        defined in this notebook.\n","    Returns\n","    -------\n","    dict\n","        Mapping from criterion name to the parsed JSON returned by the judge‚ÄëLLM.\n","    \"\"\"\n","    if criteria is None:\n","        # Use the global variable if available; otherwise default to common criteria.\n","        criteria = globals().get(\"criteria\", [\"Fluency\", \"Grammar\", \"Tone\", \"Length\", \"Grounding\"])\n","\n","    results = {}\n","    for crit in criteria:\n","        prompt = CRIT_PROMPT_TEMPLATE.format(criterion=crit, description=description)\n","        raw = call_model(prompt, model_name)\n","        try:\n","            results[crit] = json.loads(raw)\n","        except json.JSONDecodeError:\n","            raise ValueError(f\"Error parsing JSON for criterion {crit}: {raw}\")\n","\n","    return results\n","# ---------------------------------------------------------------\n"]},{"cell_type":"markdown","id":"ca557369","metadata":{"id":"ca557369"},"source":["## 4¬†Different¬†Model¬†JLM\n","\n","### Why switch models?\n","\n","We **do not reuse the generator model as the judge** because:\n","\n","1. **Self‚Äëevaluation bias** ‚Äì a model tends to justify its own outputs, consciously or not, and can over‚Äëscore itself.  \n","2. **Correlated failure modes** ‚Äì the generator‚Äôs blind‚Äëspots (hallucinations, sloppy reasoning) may also appear in its judgments.  \n","3. **Diversity of signal** ‚Äì using an *independent* model (preferably from another provider) gives a more reliable second opinion.\n","\n","### Your task\n","\n","* Pick a **different provider** than you used in Sections¬†1‚Äë3 (e.g. if you relied on OpenAI, try Gemini or Claude; if you used HF, try OpenAI, etc.).  \n","* Implement **`per_criterion_jlm_alt()`** ‚Äì a thin wrapper that calls `per_criterion_jlm()` *after temporarily switching* the backend to this alternate provider, so it still returns the usual `{explanation, verdict}` JSON for **each criterion**.  \n","  * You may use helpers such as `call_openai`, `call_gemini`, `call_hf`, or any LangChain chat model‚Äîyour choice.  \n","  * Feel free to craft a new prompt tailored to the provider.  \n","\n","### Compare\n","\n","Run both graders ‚Äì **Section¬†3‚Äôs `per_criterion_jlm()`** and the new **`per_criterion_jlm_alt()`** ‚Äì on the *same* product description:\n","\n","* Are the verdicts identical?  \n","* Does the new JLM appear to **favour or penalise** the generated answer differently?  \n","* Add a brief reflection in the markdown cell that follows.\n"]},{"cell_type":"code","execution_count":null,"id":"26864188","metadata":{"id":"26864188"},"outputs":[],"source":["#TODO STUDENT: call the JLM (using section 3 code) with a different model from the one used\n","#      to generate the descriptions and evaluate the results so far"]},{"cell_type":"markdown","id":"ab06c1f0","metadata":{"id":"ab06c1f0"},"source":["## 5¬†Creating a¬†Dataset‚ÄëEvaluation Infrastructure\n","\n","You now have several JLM graders (`structured_jlm`, `per_criterion_jlm` and `naive_jlm`) that can judge a *single* product description.  \n","The next step is to **scale that evaluation to an entire dataset** of model outputs so you can compute corpus‚Äëlevel metrics and quickly spot problematic cases.\n","\n","### Why build an infrastructure?\n","\n","* **Reproducibility¬†& tracking** ‚Äì running all descriptions through the *same* grading pipeline lets you store the verdict/explanation alongside the raw generations for future audits.  \n","* **Bulk analytics** ‚Äì with judgments in a DataFrame you can compute pass‚Äërates, per‚Äëcriterion confusion matrices, or slice‚Äëby‚Äëslice performance (e.g., electronics vs. clothing).  \n","* **Error triage** ‚Äì the explanation text helps you cluster or search for recurring failure modes (e.g., \"missing dimensions\", \"over‚Äëhyping\").  \n","* **Continuous evaluation** ‚Äì once scripted, you can rerun the notebook after each model iteration and track deltas automatically.\n","\n","### What to implement\n","\n","1. **Iterate over the dataset** ‚Äì a `pandas.DataFrame` where each row has at least a `description` column (and optionally metadata like `product_id`, `category`).  \n","2. **Apply your chosen grader** ‚Äì call **one** of your JLM functions (`per_criterion_jlm` is a good default) per row.  \n","3. **Add columns** ‚Äì for every evaluation criterion create `<criterion>_explanation` and `<criterion>_verdict` columns.  \n","4. **Save or return the enriched DataFrame** so you can inspect it, compute stats, or export to CSV/Parquet.\n","\n","A starter helper function is provided in the code cell below‚Äîcustomise it as needed."]},{"cell_type":"code","execution_count":null,"id":"2efc9b9c","metadata":{"id":"2efc9b9c"},"outputs":[],"source":["# ------------------------- STUDENT TODO -------------------------\n","# Iterate over a DataFrame of product descriptions, run alt_jlm on each row,\n","# and append explanation & verdict columns per criterion.\n","#\n","# Expected input format:\n","#   df[\"description\"]  -> the text to judge\n","#\n","# Example output columns for criterion 'clarity':\n","#   clarity_explanation , clarity_verdict\n","#\n","# Adjust variable names / saving as needed.\n","\n","import pandas as pd\n","\n","def judge_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n","    judged_rows = []\n","    for _, row in df.iterrows():\n","        desc = row[\"description\"]\n","        result = per_criterion_jlm(desc, criteria)  # {crit: {\"explanation\": ..., \"verdict\": ...}}\n","        flat = row.to_dict()\n","        for crit, vals in result.items():\n","            flat[f\"{crit}_explanation\"] = vals[\"explanation\"]\n","            flat[f\"{crit}_verdict\"] = vals[\"verdict\"]\n","        judged_rows.append(flat)\n","    return pd.DataFrame(judged_rows)\n","\n","# Example usage:\n","# df_input = pd.read_csv(\"my_descriptions.csv\")\n","# df_scored = judge_dataframe(df_input)\n","# df_scored.to_csv(\"descriptions_with_judgments.csv\", index=False)\n","# df_scored.head()\n","# ---------------------------------------------------------------\n","\n","judge_dataframe(\"YOUR_DF\")"]},{"cell_type":"markdown","id":"c47f6dad","metadata":{"id":"c47f6dad"},"source":["\n","\n","You now have a DataFrame with per‚Äëcriterion **explanations** and **verdicts**.\n","Next, add a final scoring pass that condenses those signals into one overall\n","label (`good‚ÄØ|‚ÄØok‚ÄØ|‚ÄØbad`) per description.\n","\n","### Judgement guidelines object\n","\n","Provide a dict with **exactly these criteria** and a Boolean indicating whether\n","the criterion is a **no‚Äëgo** (i.e. a *bad* verdict forces the overall label to\n","*bad*):\n","\n","| Criterion   | No‚Äëgo? |\n","|-------------|--------|\n","| Fluency     | False  |\n","| Grammar     | True?  |\n","| Tone        | False  |\n","| Length      | False  |\n","| Grounding   | True?  |\n","\n","Adjust the True/False flags to suit your project.\n","\n","```python\n","judgement_guidelines = {\n","    \"Fluency\":   False,\n","    \"Grammar\":   True,\n","    \"Tone\":      False,\n","    \"Length\":    False,\n","    \"Grounding\": True,\n","}\n","```\n","\n","### Task\n","\n","1. Implement `apply_final_scoring(df, judgement_guidelines)` (cell below).  \n","2. Ensure it adds a `final_label` column using no‚Äëgo rules plus an aggregated\n","   score from the remaining criteria."]},{"cell_type":"code","execution_count":null,"id":"465eae5f","metadata":{"id":"465eae5f"},"outputs":[],"source":["# ------------------------- STUDENT TODO -------------------------\n","# Post‚Äëprocessing: compute a row‚Äëlevel final score using guideline rules.\n","# Inputs:\n","#   df                ‚Äì the DataFrame returned by judge_dataframe()\n","#   judgement_guidelines ‚Äì dict mapping criterion -> bool\n","#                          True  => \"no‚Äëgo\" criterion (a 'bad' verdict => overall bad)\n","#                          False => normal criterion\n","#\n","# Strategy (example ‚Äì feel free to tweak):\n","#   ‚Ä¢ Map verdicts to numeric scores {good: 2, ok: 1, bad: 0}.\n","#   ‚Ä¢ If any no‚Äëgo criterion is 'bad' => overall_label = 'bad' (score 0).\n","#   ‚Ä¢ Otherwise sum numeric scores, normalise by (2 * #criteria) to get 0‚Äë1 range,\n","#       and convert back to overall_label ('good' ‚â• 0.75, 'ok' ‚â• 0.4 else 'bad').\n","\n","VERDICT_TO_SCORE = {\"good\": 2, \"ok\": 1, \"bad\": 0}\n","\n","def apply_final_scoring(df, judgement_guidelines: dict) -> pd.DataFrame:\n","    scored_rows = []\n","    crits = [c for c in judgement_guidelines.keys()]\n","    for _, row in df.iterrows():\n","        # Check no‚Äëgo first\n","        overall_label = None\n","        for crit in crits:\n","            v = row.get(f\"{crit}_verdict\", \"\").lower()\n","            if judgement_guidelines[crit] and v == \"bad\":\n","                overall_label = \"bad\"\n","                break\n","        # If not nailed by no‚Äëgo, compute aggregate\n","        if overall_label is None:\n","            total = 0\n","            max_total = 2 * len(crits)\n","            for crit in crits:\n","                total += VERDICT_TO_SCORE.get(row.get(f\"{crit}_verdict\", \"\").lower(), 0)\n","            norm = total / max_total\n","            if norm >= 0.75:\n","                overall_label = \"good\"\n","            elif norm >= 0.4:\n","                overall_label = \"ok\"\n","            else:\n","                overall_label = \"bad\"\n","        scored_row = row.to_dict()\n","        scored_row[\"final_label\"] = overall_label\n","        scored_rows.append(scored_row)\n","    return pd.DataFrame(scored_rows)\n","\n","# Example usage:\n","# guidelines = {\"clarity\": False, \"factuality\": True, \"persuasiveness\": False}\n","# df_scored = apply_final_scoring(df_scored, guidelines)\n","# df_scored.head()\n","# ---------------------------------------------------------------\n","\n","apply_final_scoring(\"YOUR_JUDGED_DF\", \"YOUR_GUIDELINES\")"]},{"cell_type":"markdown","id":"419e9ccf","metadata":{"id":"419e9ccf"},"source":["## 6¬†Majority‚ÄëVote¬†Ensemble - NOT EASY\n","\n","Ensembling is a simple way to boost robustness.  \n","Here we will take **multiple independent calls** to the **alternate judge model** you built in Section¬†4 (`per_criterion_jlm`) and\n","combine their verdicts with a majority vote.\n","\n","*Call *different* `models` several times so that each run can produce (hopefully) different explanations/verdicts.*\n","\n","### Your task\n","\n","1. Implement **`majority_vote_jlm(description, n=3)`** that:\n","   * Invokes `per_criterion_jlm(description)` with **n** different models (minimum of 3),\n","   * Return a dictionary with all results that will later be written to a pd DF.\n","2. Test it on the same product description you used in Sections¬†3‚Äì4.\n","3. Briefly comment: *Does majority voting smooth out noisy judgments?*"]},{"cell_type":"code","execution_count":null,"id":"1c8cccb3","metadata":{"id":"1c8cccb3"},"outputs":[],"source":["#NOTE : Call at least 3 different models - not same model, return all responses and then take majority vote\n","\n","# ------------------------- STUDENT TODO -------------------------\n","\n","def majority_vote_jlm(description: str, n: int = 3) -> dict:\n","    #TODO : Implement majority vote\n","    raise NotImplementedError(\"Implement majority_vote_jlm\")\n","# ---------------------------------------------------------------\n","\n","majority_vote_jlm(\"SOME_DESCRIPTION\", n)"]},{"cell_type":"markdown","id":"2a9dc46f","metadata":{"id":"2a9dc46f"},"source":["## 7¬†*(Bonus)*¬†Dataset‚ÄëLevel Majority‚ÄëVote Evaluation\n","\n","You have a row‚Äëlevel ensemble grader (`majority_vote_jlm`) that produces a more\n","stable verdict for **one** product description.  \n","In this bonus section you‚Äôll **scale** that ensemble to an entire DataFrame so\n","you can compute dataset‚Äëlevel metrics.\n","\n","### Goals\n","\n","1. **Automate majority voting across the dataset**: run `majority_vote_jlm`\n","   *n* times per description and store the combined verdicts/explanations.  \n","2. **Reuse your scoring pipeline**: pipe the enriched DataFrame through\n","   `apply_final_scoring` to get a `final_label`.  \n","3. **Compare**: how do the label distributions differ from the single‚Äëjudge\n","   evaluation in Section¬†5?\n","\n","> *Extra credit*: add timing & cost tracking so you can quantify the trade‚Äëoff."]},{"cell_type":"code","execution_count":null,"id":"cc415eb3","metadata":{"id":"cc415eb3"},"outputs":[],"source":["# ------------------------- BONUS TODO -------------------------\n","import pandas as pd\n","\n","def judge_dataframe_mv(df: pd.DataFrame, n: int = 3) -> pd.DataFrame:\n","    \"\"\"Run majority‚Äëvote JLM `n` times per description and return enriched DF.\"\"\"\n","    rows = []\n","    for _, row in df.iterrows():\n","        desc = row[\"description\"]\n","        result = majority_vote_jlm(desc, n=n)\n","        flat = row.to_dict()\n","        for crit, vals in result.items():\n","            flat[f\"{crit}_explanation\"] = vals[\"explanation\"]\n","            flat[f\"{crit}_verdict\"] = vals[\"verdict\"]\n","        rows.append(flat)\n","    return pd.DataFrame(rows)\n","\n","# Example workflow:\n","# df_mv = judge_dataframe_mv(df_input, n=5)\n","# df_mv_scored = apply_final_scoring(df_mv, judgement_guidelines)\n","# print(df_mv_scored['final_label'].value_counts())\n","# ---------------------------------------------------------------\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":5}
